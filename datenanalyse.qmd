---
# "| ![](www/logo@2x.png){width=25%} \\vspace{0rem} \n"
title: "Datenanalyse in 6 Schritten"
subtitle: "Eine Anleitung in der statistischen Programmiersprache R"
author: "Patrik Häcki"
date: "today"
date-format: "long"
abstract-title: "Version"
abstract: "4.2"
format: 
  html: 
    toc: true
    toc-depth: 4
    toc-title: Inhalt
    other-links: 
      - text: DataVisual
        href: https://www.datavisual.ch/
        target: _blank
        icon: house-door
    # code-links: repo
    html-math-method: katex
    anchor-sections: true
    smooth-scroll: true
    link-external-icon: false
    link-external-newwindow: true
    comments: false
    code-fold: show
    code-summary: "Code anzeigen"
    code-tools: 
      source: false
      toggle: true
      caption: "Code"
    code-block-bg: false
    code-block-border-left: "#9fc131"
    highlight-style: atom-one
    code-copy: true
    theme: 
      - sandstone
      - css/custom.scss
    lightbox: true
  # pdf:
    # toc: true
    # toc-depth: 4
    # documentclass: scrreprt
    # papersize: a4
    # margin-top: 20mm
    # margin-left: 20mm
    # number-sections: true
    # shift-heading-level-by: -1
    # highlight-style: pygments
    # colorlinks: true
    # fig-align: left
    # fig-width: 5
    # fig-height: 3.8
    # fontsize: 11pt
    # df-print: tibble
    # cite-method: biblatex
editor: visual
lang: de
---

# Einführung

## Was ist R?

R ist eine kostenlose Open Source-Software für statistische Datenverarbeitung, die über die Website <https://stat.ethz.ch/CRAN> bezogen werden kann. Dabei umfasst R zum einen eine Vielzahl an Möglichkeiten zur Verarbeitung und Auswertung von Daten, die sich ohne grossen Aufwand nutzen lassen. Zum anderen kann man statistische Verfahren auch selbst programmieren und R fast beliebig erweitern. Von Anwendern erstellte Erweiterungen werden als Pakete oder packages bezeichnet und von ihren Programmierern oftmals für alle zugänglich gemacht. Im Comprehensive R Archive Network (kurz: CRAN), einem Netz aus Webservern, die Pakete und Code für R bereitstellen, sind eine Vielzahl solcher Pakete gelistet. Daneben wird auch Base R durch ein Kern-Team von Entwicklern ständig weiterentwickelt. R ist open-source, d.h. der Source Code ist unter der GNU Public License frei verfügbar.

## Vorteile von R

Die wesentlichen Vorteile von R lassen sich insgesamt wie folgt zusammenfassen:

-   R kann kostenlos heruntergeladen und installiert werden.

-   R steht für Windows-, Unix- und Mac-Systeme zur Verfügung.

-   R wird von einem Kern-Team von Entwicklern ständig weiterentwickelt.

-   Es gibt eine Vielzahl von frei zugänglichen Erweiterungen, die von der kontinuierlich wachsenden R-Community erstellt werden.

-   R kann durch den Nutzer selbst erweitert werden.

Aufgrund dieser Vorteile findet R zunehmend Verbreitung und wird nicht nur im wissenschaftlichen Bereich, sondern auch für Anwendungen in der Wirtschaft eingesetzt.

## Installation

### R

Zentrale Anlaufstelle für den Download von R, für Zusatzpakete sowie für frei verfügbare Literatur ist die R-Projektseite <https://www.r-project.org> (in Englisch) oder das Comprehensive R Archive Network für die Schweiz [https://stat.ethz.ch/CRAN](https://stat.ethz.ch/CRAN/){.uri}, welches von der ETH Zürich betreut wird.

### R-Editor

Anders als manche seiner kommerziellen und kostenpflichtigen Konkurrenten (wie etwa SPSS) kommt die freie Programmiersprache R ohne grafische Benutzeroberfläche daher. Nach dem Download und der Installation von R ist es deshalb empfehlenswert, zusätzlich einen komfortableren, kostenlosen R-Editor zu installieren.

-   RStudio von Posit (<https://posit.co>) ist die wohl am weitesten verbreitete integrierte Entwicklungsumgebung (IDE) für die Programmiersprache R. Weitere nützliche Editoren sind
-   Jupyter Notebooks von <https://jupyter.org> oder
-   der kostenlose Quelltext-Editor Visual Studio Code von Microsoft (<https://code.visualstudio.com>).

# Datenanalyse

## Pakete laden

R-Pakete sind Sammlungen von Funktionen und Werkzeugen, die von der R-Community entwickelt wurden. Sie erhöhen die Leistungsfähigkeit von R, indem sie bestehende Basisfunktionen verbessern oder neue Funktionen hinzufügen.

Mit der Funktion `install.packages()` werden neue Pakete installiert (z.B. das Paket janitor).

```{r}
#| message: false
#| echo: false

# Pakete laden
library(showtext)
library(sysfonts)
library(corrplot)
library(ggdist)
library(ggforce)
library(glue)
library(GWalkR)
library(janitor)
library(paletteer)
library(palmerpenguins)
library(patchwork)
library(psych)
library(RColorBrewer)
library(readxl)
library(recipes)
library(rjson)
library(scales)
library(skimr)
library(summarytools)
library(statip)
library(tidyAML)
library(TidyDensity)
library(treemap)
library(tidyverse)
```

Für die Datenanalyse in sechs Schritten laden Sie bitte folgende Pakete in die aktuelle R-Session:

-   corrplot
-   ggdist
-   ggforce
-   glue
-   janitor
-   paletteer
-   palmerpenguins
-   patchwork
-   psych
-   RColorBrewer
-   readxl
-   recipes
-   rjson
-   scales
-   skimr
-   summarytools
-   statip
-   tidyAML
-   TidyDensity
-   tidyverse
-   treemap

```{r}
#| message: false
#| echo: false

# Schriftart «Manrope» laden
font_add_google(name = "Manrope", family = "Manrope")
```

```{r}
#| message: false
#| echo: false

# Showtext automatisch für neue Plots verwenden
showtext_auto()
```

## Daten laden

### Beispieldaten

In R stehen zahlreiche Import-Funktionen zur Verfügung, um Daten aus unterschiedlichen Anwendungen und in verschiedensten Formaten zu laden.

Zur Veranschaulichung der verschiedenen Funktionen und Visualisierungen werden die folgenden Datensätze verwendet:

-   [Penguins](https://allisonhorst.github.io/palmerpenguins/)

-   [mtcars](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/mtcars) (Wird aus­schliesslich im Abschnitt «[Ausblick auf Machine Learning]» verwendet.)

-   [Lernende nach Grossregion, Schulkanton, Geschlecht, Staatsangehörigkeit und Charakter der Schule](https://opendata.swiss/en/dataset/sekundarstufe-ii-berufliche-grundbildung-lernende-nach-grossregion-schulkanton-geschlecht-staat1) des Bundesamts für Statistik BFS

-   [Lernende nach Grossregion, Schulkanton, Ausbildungsform und Bildungstyp](https://opendata.swiss/en/dataset/sekundarstufe-ii-berufliche-grundbildung-lernende-nach-grossregion-schulkanton-ausbildungsform-1) des Bundesamts für Statistik BFS

```{r}
#| message: false
#| echo: false

setwd(dir = "~/Nextcloud/Diverses/Websites/DataVisual/R/Datenanalyse")
```

```{r}
bgb_staat <- 
  read_xlsx(path = "data/je-d-15.02.02.01.03.xlsx", 
            sheet = 1, 
            col_names = c("Schulkanton", "Total", "Geschlecht_Mann", "Geschlecht_Frau", 
                          "Staatsangehörigkeit_Schweiz", "Staatsangehörigkeit_Ausland", 
                          "Staatsangehörigkeit_Unbekannt", "Charakter der Schule_Öffentlich", 
                          "Charakter der Schule_Privat subventioniert", 
                          "Charakter der Schule_Privat nicht subventioniert"), 
            trim_ws = TRUE, 
            skip = 6)

bgb_typ <- 
  read_xlsx(path = "data/je-d-15.02.02.01.02.xlsx", 
            sheet = 1, 
            col_names = c("Schulkanton", "Total", "Ausbildungsform_Vollschulisch", 
                          "Ausbildungsform_Dual", "Typ_EFZ", "Typ_EBA", 
                          "Typ_Nicht BBG-reglementierte berufliche Grundbildung"), 
            trim_ws = TRUE, 
            skip = 6)
```

## Deskriptive Statistik

### Explorative Datenanalyse

Die explorative Datenanalyse (Exploratory Data Analysis, abgekürzt EDA) ist ein wesentlicher Schritt in jedem Datenanalyseprojekt. Sie dient der Analyse und Untersuchung von Datensätzen und der deskriptiven Zusammenfassung ihrer wichtigsten Merkmale, wobei oft grafische Darstellungsmethoden verwendet werden. Mit Hilfe von Tabellen, Grafiken und der Ermittlung relevanter Kennzahlen wird versucht, einen Überblick über das gesamte Datenmaterial zu gewinnen, es zu ordnen und zusammenzufassen. Die EDA bildet damit die Grundlage für die weitere Analyse.

### Objekte

Die Funktion `ls()` liefert eine Liste aller bisher gespeicherten Objekte wie Daten und Funktionen.

```{r}
# Gespeicherte Objekte anzeigen
ls()
```

Mit der Funktion `rm()` werden alle unerwünschten Dateien gelöscht.

```{r}
# Unerwünschte Dateien entfernen
rm(bgb_typ)
```

Es ist auch möglich, alle Objekte auf einmal zu entfernen.

```{r}
# Alle Objekte löschen
rm(list = ls())
```

### Datentypen

Ein Datensatz kann Merkmale unterschiedlicher Datentypen enthalten. Einige Daten können Zahlen sein (z.B. Alter oder Gewicht), während andere aus Text bestehen (wie Name oder Adresse). R kennt die folgenden Haupttypen:

-   Numerisch: Zahlen, einschliesslich Ganzzahlen (ganze Zahlen) und Dezimalzahlen

-   Zeichen: Textstrings, wie Wörter oder Sätze

-   Logisch: Wahr- oder Falsch-Werte

-   Faktor: Kategoriale Daten mit definierten Stufen (z.B. Farben: rot, grün, blau).

Die Funktion `class()` bietet einen allgemeinen Überblick über den Datentyp, wie z.B. «numerisch» oder «Zeichen».

```{r}
# Datentyp anzeigen
class(penguins$bill_length_mm)
```

Die Funktion `typeof()` zeigt spezifischere Details innerhalb des Datentyps (beispielsweise «double» für Dezimalzahlen in «numerisch»).

```{r}
# Detaillierter Datentyp anzeigen
typeof(penguins$bill_length_mm)
```

### Anfang und Ende

Mit der Funktion `slice_head()` werden die ersten Zeilen bzw. Beobachtungen ausgegeben. In diesem Beispiel wurde die Anzahl auf 10 festgelegt. Der Wert kann jedoch flexibel gewählt werden.

```{r}
slice_head(.data = penguins, 
           n = 10)
```

Die Funktion `first()` gibt das erste Element eines Eingabevektors zurück.

```{r}
# Erstes Datenelement ausgeben
first(x = penguins)
```

Selbstverständlich ist es in R auch möglich, die letzten Zeilen eines Data Frames (dt. Datenrahmen) auszugeben. Die Funktion `slice_tail()` gibt die letzten n Zeilen eines Datenrahmens zurück (Standardwert ist 6).

```{r}
slice_tail(.data = penguins, 
           n = 5)
```

Verwenden Sie `slice_tail(1)`, um nur die letzte Zeile zu erhalten.

```{r}
slice_tail(.data = penguins, 
           n = 1)
```

Die Funktion `last()` ergänzt die Funktion `first()`, indem sie ebenfalls das letzte Element eines Vektors zurückgibt.

```{r}
# Letztes Datenelement ausgeben
last(x = penguins)
```

Mit der Funktion `slice()` von dplyr können bestimmte Zeilen ausgewählt werden. Um die vorletzte Zeile zu bekommen, verwenden Sie `n()` -1.

```{r}
slice(.data = penguins, 
      n() - 1)
```

Wenn Sie statt der ersten oder letzten Zeile eine zufällige Auswahl von Zeilen ausgeben möchten, steht Ihnen dafür die Funktion `slice_sample()` zur Verfügung.

```{r}
slice_sample(.data = penguins, 
             n = 5)
```

Mit der Funktion `nth()` kann ein Vektorelement an einer beliebigen Stelle innerhalb des Vektors extrahiert werden. Durch Angabe des entsprechenden Elements erhalten Sie die gewünschte Ausgabe.

```{r}
nth(x = penguins, 
    n = 7)
```

Durch das Voranstellen eines Minuszeichens vor die Position lassen sich Elemente vom Ende des Vektors abrufen.

```{r}
nth(x = penguins, 
    n = -7)
```

Es ist auch möglich, nur die Namen der Spalten auszugeben.

```{r}
# Spaltennamen ausgeben
names(penguins)
```

### Umfang

```{r}
#| message: false
#| echo: false

bgb_staat <- 
  read_xlsx(path = "data/je-d-15.02.02.01.03.xlsx", 
            sheet = 1, 
            col_names = c("Schulkanton", "Total", "Geschlecht_Mann", "Geschlecht_Frau", 
                          "Staatsangehörigkeit_Schweiz", "Staatsangehörigkeit_Ausland", 
                          "Staatsangehörigkeit_Unbekannt", "Charakter der Schule_Öffentlich", 
                          "Charakter der Schule_Privat subventioniert", 
                          "Charakter der Schule_Privat nicht subventioniert"), 
            trim_ws = TRUE, 
            skip = 6)

bgb_typ <- 
  read_xlsx(path = "data/je-d-15.02.02.01.02.xlsx", 
            sheet = 1, 
            col_names = c("Schulkanton", "Total", "Ausbildungsform_Vollschulisch", 
                          "Ausbildungsform_Dual", "Typ_EFZ", "Typ_EBA", 
                          "Typ_Nicht BBG-reglementierte berufliche Grundbildung"), 
            trim_ws = TRUE, 
            skip = 6)
```

Interessieren Sie sich für den Umfang Ihres Datensatzes? Die Basisfunktion `dim()` liefert die Anzahl der Zeilen und Spalten.

```{r}
# Umfang des Datensatzes
dim(penguins)
```

Sobald die Daten geladen sind, können Sie mit `names()` oder `colnames()` die Variablennamen überprüfen.

```{r}
# Variablennamen prüfen
names(penguins)
```

Der gesamte Datensatz kann mit der Funktion `View()` angezeigt werden. Die Darstellung ähnelt der von Microsoft Excel.

```{r}
# Datensatz anzeigen
View(penguins)
```

Während `View()` eine Excel-ähnliche Darstellung bietet, ermöglicht die Funktion `fix()` das Editieren von Datenzellen vergleichbar wie in Excel.

```{r}
# Datenzellen editieren
fix(penguins)
```

Mit `glimpse()` können Sie eine transponierte Version des Datenrahmens anzeigen, bei der die Spalten vertikal und die Daten horizontal dargestellt werden. `glimpse()` zeigt die Dimension des Datenrahmens und der zugrunde liegende Datentyp jedes Merkmals.

```{r}
# Zusammenfassung der wichtigsten Kennzahlen in transponierter Form
glimpse(penguins)
```

Alternativ kann die Struktur der Daten auch mit der Funktion `str()` ermittelt werden.

```{r}
# Datenstruktur anzeigen
str(penguins)
```

### Masse der zentralen Tendenz

Um die Verteilung der Daten besser zu verstehen, können Sie die so genannten Masse der zentralen Tendenz untersuchen, welche statistisch die Mitte der Daten beschreibt. Ziel ist es, einen typischen Wert zu finden. Gängige Methoden zur Bestimmung der Datenmitte sind:

-   Mittelwert: Ein einfacher Durchschnittswert, der berechnet wird, indem alle Werte des Stichprobensatzes addiert und dann die Gesamtsumme durch die Anzahl der Stichproben dividiert wird.

-   Median: Der Wert, der in der Mitte des Bereichs aller Stichprobenwerte liegt.

-   Modus: Der am häufigsten vorkommende Wert in der Stichprobenmenge.

```{r}
mean(penguins$body_mass_g, na.rm = TRUE)
median(penguins$body_mass_g, na.rm = TRUE)
mfv(penguins$body_mass_g, na_rm = TRUE)

head(table(penguins$body_mass_g), n = 10) # Alternative: Paket «tadaatoolbox»
```

Mit der Funktion `colMeans()` von Base R können die Mittelwerte mehrerer metrischer Vektoren auf einmal berechnet werden.

```{r}
colMeans(x = penguins[, 3:6], 
         na.rm = TRUE)
```

#### Apply-Familie

Schleifen sind grossartig, aber für sich wiederholende Aufgaben mit Datenstrukturen ist die Vektorisierung unschlagbar. Sie ist schneller, sauberer und ermöglicht es Ihnen, sich auf das «Was» statt auf das «Wie» Ihrer Analyse zu konzentrieren. Hier kommt die Apply-Funktion ins Spiel.

Der Mittelwert (FUN) wird mit `lapply()`auf die ausgewählten Spalten angewendet und als Liste zurückgegeben.

```{r}
lapply(X = na.omit(penguins[, 3:6]), 
       FUN = mean)
```

`sapply()` ist vergleichbar mit `lapply()`, versucht aber die Ausgabe zu vereinfachen. Sind alle Ergebnisse vom gleichen Typ (z.B. numerisch), ist die Rückgabe ein Vektor anstelle einer Liste.

```{r}
sapply(X = na.omit(penguins[, 3:6]), 
       FUN = mean)
```

Weitere Apply-Funktionen finden Sie im Abschnitt «[Gruppierte Zusammenfassung]».

### Streuungsmasse

Wie gross ist die Variabilität in den Daten? Zu den typischen Statistiken, welche die Variabilität messen, gehören:

-   Spannweite (Range): Die Differenz zwischen dem Maximum und Minimum. Dafür gibt es keine eigene Funktion, aber sie lässt sich leicht mit den Funktionen `min()` und `max()` berechnen. Ein anderer Ansatz ist die Verwendung der Funktion `range()` von Base R, welche einen Vektor zurückgibt, der das Minimum und Maximum aller angegebenen Argumente enthält. Wenn Sie diese mit `diff()` umschliessen, können Sie die Spannweite berechnen.

-   Varianz: Entspricht dem Mittelwert der quadrierten Differenz zum Mittelwert. Sie können die eingebaute Funktion `var()` verwenden, um die Varianz zu ermitteln.

-   Standardabweichung: Entspricht der Quadratwurzel der Varianz. Sie können die integrierte Funktion `sd()` verwenden, um die Standardabweichung zu finden.

Die Position des grössten Wertes kann mit `which.max()` ermittelt werden. Für das Minimum gibt es das entsprechende Pendant mit `which.min()`.

```{r}
max(penguins$bill_length_mm, na.rm = TRUE)
which.max(penguins$bill_length_mm)
```

Durch die Verwendung der Funktion `map()` können Sie viele for-Schleifen durch Code ersetzen, der sowohl kürzer als auch einfacher zu lesen ist.

```{r}
# Spalten auswählen, um das Mass der Varianz zu analysieren
cols <- penguins %>% 
  select(c(bill_length_mm, bill_depth_mm)) %>% 
  drop_na()
```

```{r}
# Eine Funktion auf jede Spalte anwenden
map(cols, function(column) {
  range <- diff(range(column)) # dasselbe wie: max(Spalte) - min(Spalte)
  var <- var(column)
  std <- sd(column)
  glue(
    '- Spannweite: {format(round(range, 2), nsmall = 2)}
    - Varianz: {format(round(var, 2), nsmall = 2)}
    - Standardabweichung: {format(round(std, 2), nsmall = 2)}',
    .sep = '\n'
  )
})
```

#### map

Die Funktion `map()` kann auch zur Textmanipulation verwendet werden.

```{r}
# Namen in Grossbuchstaben umwandeln
map(penguins$species, toupper) %>% 
  sample(size = 5)
```

```{r}
# Zusätzliche Argumente übergeben
map(penguins$species, substr, start = 1, stop = 3) %>% 
  sample(size = 5)
```

#### quantile

Mit `quantile()` kann man die Streuung bzw. die Quantile einer Variablen bestimmen.

```{r}
# Streuung von Variablen darstellen
quantile(penguins$bill_length_mm, na.rm = TRUE)
```

### Doppelte Werte

Es ist immer möglich, dass Datensätze doppelte Einträge aufweisen. Deshalb ist es wichtig, dies zu prüfen.

```{r}
# Duplikate im Datensatz prüfen
any(penguins[duplicated(penguins), ])
```

```{r}
# Duplikate in bestimmten Spalten prüfen
any(duplicated(x = penguins$species))
```

```{r}
# Duplikate in mehreren Spalten prüfen
any(duplicated(x = penguins[c("species", "island")]))
```

```{r}
# NA-Werte vom Vergleich ausschliessen
# duplicated(x = penguins, incomparables = NA)
```

Nach der Identifizierung von Dubletten besteht der nächste Schritt oft darin, diese zu entfernen.

```{r}
# Duplikate entfernen mit «duplicated»
ergebnis <- (penguins[!duplicated(penguins), ])
head(ergebnis)
```

```{r}
# Duplikate schnell entfernen mit «unique»
ergebnis <- unique(x = penguins)
head(ergebnis)
```

### Zusammenfassung von Kennwerten

Die Funktion `summary()` ermöglicht einen Überblick zu den wichtigsten Kennzahlen eines Datensatzes. Bei numerischen Merkmalen umfassen diese Minimum, 1. Quartil, Median, Mittelwert, 3. Quartil und Maximum.

```{r}
# Zusammenfassung der wichtigsten Kennzahlen
summary(penguins)
```

Die Funktion `describe()` des Pakets psych liefert ebenfalls eine Zusammenfassung der deskriptiven Statistik. Sie enthält neben den üblichen Lagemassen auch Werte für Schiefe und Kurtosis.

```{r}
describe(penguins$bill_length_mm, 
         na.rm = TRUE)
```

Mit der Funktion `summarise()` können beliebige Kennwerte ausgegeben werden. Beispielsweise können Sie das arithmetische Mittel (Mittelwert) und den Median einer numerischen Variablen finden.

```{r}
summarise(.data = penguins, 
          "Mittelwert Gewicht" = mean(body_mass_g, na.rm = TRUE), 
          "Median Gewicht" = median(body_mass_g, na.rm = TRUE))
```

#### summarytools

Das R-Paket summarytools vereinfacht den Prozess der Datenexploration, indem es Funktionen bereitstellt, die mit minimalem Code umfangreiche Zusammenfassungen Ihrer Daten erzeugen.

Die Funktion `dfSummary()` liefert eine detaillierte Zusammenfassung, einschliesslich

-   Datentypen

-   Fehlende Werte

-   Eindeutige Werte

-   Grundlegende Statistiken

-   Grafische Darstellungen

```{r}
dfSummary(x = penguins, 
          graph.col = TRUE, 
          graph.magnif = 0.75, 
          style = "grid") # %>% stview() Interaktiver HTML-Bericht
```

Verwenden Sie die Funktion `descr()`, um detaillierte deskriptive Statistiken für Ihre numerischen Variablen zu erhalten. Sie können auswählen, welche Statistiken generiert werden sollen (z.B. «common», «fivenum», usw.).

```{r}
descr(x = penguins, 
      stats = "fivenum")
```

Für kategoriale Variablen erzeugt die Funktion `freq()` Häufigkeitstabellen, welche die Verteilung der Kategorien zeigen. Dies kann Ihnen helfen, die Verteilung und Häufigkeit jeder Kategorie in Ihren Daten zu verstehen.

```{r}
# Häufigkeitsstatistik
freq(x = penguins)
```

Die explorative Datenanalyse (EDA) ist entscheidend, um Ihre Daten zu verstehen, Trends zu erkennen und Probleme aufzuspüren, bevor Sie ausführlichere Analysen durchführen. Ohne die richtigen Werkzeuge kann EDA jedoch zeitaufwendig sein. Mit dem [DataExplorer](https://dtavis.shinyapps.io/dataexplorer/) steht Ihnen eine schnellere und effizientere Methode zur Verfügung, um Ihre Daten zu analysieren und statistische Kennwerte zusammenzufassen.

#### skimr

Die Kernfunktion von skimr ist `skim()`, die für die Arbeit mit (gruppierten) Datenrahmen entwickelt wurde. Wie `summary()` zeigt `skim()` Statistiken bzw. Ergebnisse für jede Spalte.

```{r}
# Zusammenfassung der wichtigsten Kennzahlen und fehlenden Werte
skim(penguins)
```

#### GWalkR

Ein neuer Ansatz zur explorativen Datenanalyse in R ist das Paket «[GWalkR](https://github.com/Kanaries/GWalkR)». Es kombiniert das Paket «htmlwidgets» mit der JavaScript-Bibliothek «Graphic Walker» und verwandelt so den Datenrahmen in eine Tableau-ähnliche Drag&Drop-Benutzeroberfläche. Für alle, die bereits mit Datenvisualisierungssoftware wie Microsoft Power BI oder Tableau gearbeitet haben, bietet GWalkR einen intuitiven Einstieg in die Datenanalyse mit R.

```{r}
#| message: false
#| echo: false

# Tableau-ähnliche explorative Datenanalyse
# gwalkr(penguins)
```

#### Weitere Pakete

Weitere umfassende und teilweise interaktive Einblicke in importierte Datensätze liefern die nachfolgenden Pakete. Diese bieten unter anderem auch eine erste gute Zusammenfassung der fehlenden Werte (n_missing).

-   [esquisse](https://cran.r-project.org/web/packages/esquisse/vignettes/get-started.html): RStudio-Add-In für interaktive Datenvisualisierung

-   [radiant](https://radiant-rstats.github.io/docs/): Browserbasierte Schnittstelle für Analysen in R, basierend auf dem shiny-Paket

-   [explore](https://cran.r-project.org/web/packages/explore/vignettes/explore.html): Vereinfacht die explorative Datenanalyse für bivariate Analysen

-   [dataxray](https://agstn.github.io/dataxray/): Interaktive Tabellenschnittstelle für Zusammenfassungen von Daten

### Gruppierte Zusammenfassung

Wenn Sie wissen möchten, wie viele Pinguine auf den drei Inseln untersucht wurden, können Sie die Funktion `table()` verwenden. Den prozentualen Anteil erhält man mit `prop.table()`.

```{r}
# Tabelle ausgeben
table(penguins$island)
```

```{r}
# Prozentwerte ausgeben
round(x = prop.table(table(penguins$island)) * 100, digits = 2)
```

Mit den Argumenten `margin = 1` oder `margin = 2` können die relativen Zeilen- oder Spaltenprozentsätze berechnet werden.

```{r}
# Prozentwerte pro Zeile
round(x = prop.table(table(penguins$species, penguins$island), 
                     margin = 1) * 100, 
      digits = 2)
```

```{r}
# Prozentwerte pro Spalte
round(x = prop.table(table(penguins$species, penguins$island), 
                     margin = 2) * 100, 
      digits = 2)
```

Die Funktion `addmargins()` fügt die Randsummen hinzu.

```{r}
# Randsummen ausgeben
addmargins(table(penguins$species, penguins$island))
```

Manchmal ist es sinnvoll, die Tabelle zu transponieren (Spalten und Zeilen zu tauschen).

```{r}
table(penguins$species, penguins$sex)

t(table(penguins$species, penguins$sex)) # Transponierte Tabelle
```

Die Ausgabe der Absolutwerte ist auch mit Hilfe der Funktion `count()` möglich.

```{r}
count(x = penguins, 
      species, 
      island, 
      sort = TRUE)
```

### Grafische Darstellung

Grafische Darstellungen ermöglichen eine schnelle und einfache Interpretation von Daten, indem sie Trends und Muster visuell hervorheben. Zudem bieten sie einen klaren Überblick über grosse Datenmengen und erleichtern es, wichtige Informationen auf einen Blick zu erfassen. Sie unterstützen die effektive Kommunikation komplexer Daten, was besonders in Präsentationen und Berichten nützlich ist.

Die Funktion `colors()` gibt einen Vektor zurück, der alle in R eingebauten Farbnamen in alphabetischer Reihenfolge enthält, wobei das erste Element «white» ist.

```{r}
head(colors(), n = 20)
```

#### Säulendiagramm

Das Säulendiagramm dient dazu, die Werte unterschiedlicher Kategorien oder Gruppen gegenüberzustellen. Auf der x-Achse sind die Kategorien oder Gruppen dargestellt, während die y-Achse die entsprechenden Werte anzeigt.

```{r}
# Nominalskalierte (kategoriale) oder metrische Variable
plot(penguins$species, 
     col = "grey", 
     border = NA, 
     ylim = c(0, 200))
```

```{r}
# Häufigkeitstabellen mit der Plotfunktion von Base R darstellen
barplot(table(penguins$species), 
        col = "grey", 
        border = NA, 
        main = "Häufigkeitstabelle", 
        ylim = c(0, 200))
```

```{r}
# Gruppiertes Säulendiagramm
barplot(prop.table(table(penguins$species, penguins$sex), margin = 2), 
        legend.text = TRUE, 
        beside = TRUE, 
        xlim = c(0, 11), 
        ylim = c(0, 0.5))
```

```{r}
# Mittlere Schnabellänge für die drei Pinguinarten berechnen
mittlere_schnabellaenge <- 
  tapply(X = penguins$bill_length_mm, 
         INDEX = penguins$species, 
         FUN = mean, na.rm = TRUE)

# Säulen mit unterschiedlichen Farben
barplot(mittlere_schnabellaenge, 
        col = c("#9FC131", "#93257B", "#57AF2C"), 
        ylim = c(0, 50))
```

#### Histogramm

Ein Histogramm dient zur Darstellung der Verteilung kontinuierlicher Daten, wie beispielsweise Mess- oder Zeitdaten. Auf der x-Achse werden die Werte des Datensatzes abgebildet, während die y-Achse die Häufigkeit zeigt, d.h. wie oft jeder Wert im Datensatz vorkommt.

```{r}
hist(x = penguins$body_mass_g, 
     breaks = 20, 
     col = "grey", 
     border = "white", 
     main = "Gewichtsverteilung der Pinguine", 
     xlim = c(2000, 7000), 
     ylim = c(0, 50), 
     xlab = "Gewicht in Gramm", 
     ylab = "Häufigkeit")
```

#### Dichteplot

Der Dichteplot ist eine grafische Darstellung der Verteilung einer numerischen Variablen und verwendet die Kerndichteschätzung, um eine glatte Kurve zu erzeugen. Dies ermöglicht eine kontinuierliche und detaillierte Ansicht der Datenverteilung im Vergleich zu einem Histogramm.

```{r}
options(scipen = 999)

plot(density(x = penguins$body_mass_g, 
             na.rm = TRUE))
```

#### Boxplot

Ein Boxplot, auch Box-and-Whisker-Plot genannt, ist ein Diagramm zur grafischen Darstellung von Datenverteilungen. Er zeigt den Median, die Quartile und mögliche Ausreisser, wodurch man schnell einen Überblick über die Verteilung und Streuung der Daten erhält.

```{r}
boxplot(penguins$body_mass_g ~ penguins$species)
```

```{r}
boxplot(penguins$bill_length_mm, 
        penguins$bill_depth_mm, 
        names = c("Schnabellänge", "Schnabelbreite"))
```

#### Streudiagramm

Das Streudiagramm, auch Scatterplot genannt, ist ein Diagramm, das die Beziehung zwischen zwei Variablen darstellt. Jeder Punkt im Diagramm repräsentiert ein Datenpaar, wodurch Muster, Trends oder Korrelationen zwischen den Variablen sichtbar werden.

```{r}
plot(penguins$bill_depth_mm, penguins$bill_length_mm, 
     main = "Gegenüberstellung von Schnabelbreite und Schnabellänge")

```

#### Streumatrix

Die Funktion `pairs.panels()` erstellt eine Streumatrix, welche die paarweisen Beziehungen zwischen mehreren Variablen in einem Datensatz darstellt. Jede Zelle der Matrix zeigt ein Streudiagramm für ein Variablenpaar, was es ermöglicht, Muster und Korrelationen zwischen allen Variablen gleichzeitig zu erkennen.

```{r}
pairs.panels(x = penguins[, 3:6], 
             hist.col = "#9FC131")
```

Die Funktion `ggpairs()` aus dem Paket GGally visualisiert ebenfalls paarweise Beziehungen.

## Datenaufbereitung

### Daten bereinigen

Die Bedeutung der Datenbereinigung wird häufig unterschätzt. Dabei ist sie ein grundlegender Schritt für eine erfolgreiche Datenanalyse. In vielen Fachportalen und Artikeln wird darauf hingewiesen, dass die Datenbereinigung nach dem Pareto-Prinzip ca. 80% der Zeit einer Datenanalyse in Anspruch nimmt und die eigentliche Analyse nur 20%.

Nachdem Sie Ihre Rohdaten importiert und sich einen ersten Überblick verschafft haben, ist es immer eine gute Idee, diese zu bereinigen. Dadurch werden Fehler und andere Probleme reduziert. Dabei werden fehlerhafte Datenpunkte entfernt oder die Daten in ein nützlicheres Format konvertiert. In anderen Situationen können Datenpunkte, die deutlich ausserhalb des erwarteten Bereichs liegen, auch Ausreisser genannt, manchmal aus Analysen entfernt werden. Dies sollte jedoch sorgfältig geprüft werden, um sicherzustellen, dass keine Datenpunkte gelöscht werden, die echte Informationen liefern.

### Verzerrungen (Bias)

Ein weiteres häufiges Problem bei realen Daten sind Verzerrungen (Bias). «Verzerrung» bezieht sich auf eine menschliche Neigung, bestimmte Arten von Werten häufiger als andere auszuwählen, und zwar auf eine Weise, welche die zugrunde liegende Gesamtheit (Population) der «realen Welt» fehlerhaft darstellt. Verzerrungen lassen sich manchmal identifizieren und verhindern, indem Sie sich bei der Untersuchung von Daten vor Augen halten, woher diese stammen.

### Pipe-Operator

R ist eine funktionale Sprache, was bedeutet, dass der Code oft viele Klammern enthält. Bei komplexem Code bedeutet dies, dass diese Klammern ineinander verschachtelt werden müssen. Dadurch ist der R-Code schwer zu lesen und zu verstehen. Hier kommt der Pipe-Operator ins Spiel.

Pipe ist ein Infix-Operator, der im Paket magrittr (Bestandteil von tidyverse) von Stefan Milton Bache eingeführt wurde. Er wird verwendet, um die Ausgabe einer Funktion als Eingabe an eine andere Funktion weiterzuleiten, was den Code im Idealfall leicht lesbar und effizient macht. Mit anderen Worten: Der Pipe-Operator `%>%` wird verwendet, um eine Folge von mehreren Operationen auf elegante Weise auszudrücken und die Abläufe intuitiver zu gestalten.

```{r}
penguins %>% 
  filter(body_mass_g == 2900)
```

Der Pipe-Operator kann wie folgt als Arbeitsanweisung formuliert werden: «Nehmen Sie den Datensatz «penguins» **UND DANN** filtern Sie nach Gewicht ist gleich 2850g.»

### Daten mit janitor bereinigen

Bestehende Spaltennamen sind oftmals intuitiv und leicht verständlich, aber nicht unbedingt einfach im Code zu handhaben. Mit der Funktion `clean_names()` aus dem Paket janitor können Sie Spaltennamen mühelos bereinigen. Sie können wählen, ob Sie alle Namen in Snake Case (alle Wörter klein geschriebenen, getrennt durch Unterstriche), Variationen von Camel Case (Grossbuchstaben zwischen den Wörtern), Title Case oder andere Stile ändern möchten. Weiter werden Leerzeichen in \_ umgewandelt und Klammern entfernt. Auf diese Weise sind die Spaltenbezeichnungen leicht verständlich und gut im Code zu verarbeiten.

Datensätze mit leeren oder überflüssigen Zeilen oder Spalten sind keine Seltenheit. Dies gilt insbesondere für Excel-Dateien, die viele leere Zellen enthalten. Diese können mit der Funktion `remove_empty()` entfernt werden. Ohne Argument werden standardmässig sowohl Zeilen als auch Spalten mit `remove_empty()` gelöscht. Das kann man anpassen, indem man z.B. which = «rows» oder which = «cols» verwendet.

```{r}
#| message: false
#| echo: false

bgb_staat_clean <- bgb_staat %>% 
  clean_names() %>% 
  remove_empty(which = c("rows", "cols"))
```

```{r}
#| message: false
#| echo: false
#| warning: false

bgb_staat_clean <- bgb_staat_clean %>% 
  subset(select = total:charakter_der_schule_privat_nicht_subventioniert) %>% 
  mutate(across(where(is.character), as.numeric)) %>% 
  cbind(bgb_staat_clean$schulkanton) %>% 
  rename(schulkanton = 'bgb_staat_clean$schulkanton')
```

```{r}
bgb_typ_clean <- bgb_typ %>% 
  clean_names() %>% 
  remove_empty()
```

```{r}
#| warning: false

# Numerische Vektoren transformieren
bgb_typ_clean <- bgb_typ_clean %>% 
  subset(select = total:typ_nicht_bbg_reglementierte_berufliche_grundbildung) %>% 
  mutate(across(where(is.character), as.numeric)) %>% 
  cbind(bgb_typ_clean$schulkanton) %>% 
  rename(schulkanton = 'bgb_typ_clean$schulkanton')
```

Spalten, die in jeder Zeile denselben Wert enthalten, werden mit `remove_constant()` entfernt.

Die Funktion `round_half_up()` kann zum Runden von Werten auf ganze Zahlen verwendet werden.

```{r}
round_half_up(x = penguins$bill_depth_mm, digits = 0) %>% 
  head(n = 10)
```

`round_to_fraction()` wird verwendet, um auf einen beliebigen Bruch zu runden. Im Beispiel unten wurden die Zahlen auf die nächsten Viertel gerundet (Nenner = 4).

```{r}
round_to_fraction(x = penguins$bill_depth_mm, denominator = 4) %>% 
  head(n = 10)
```

### Fehlende Werte finden

Der Umgang mit fehlenden Daten ist eine häufige Herausforderung bei der Datenanalyse und bei Projekten des maschinellen Lernens. In R werden fehlende Werte mit NA (englische Abkürzung für «Not Available») gekennzeichnet. Bei der Arbeit mit Datensätzen ist es wichtig, NA-Werte zu identifizieren und angemessen zu behandeln, um eine verzerrte Analyse oder falsche Ergebnisse zu vermeiden.

```{r}
# Gibt den Wert TRUE (wahr) oder FALSE (falsch) zurück
anyNA(penguins)
```

```{r}
#| message: false
#| echo: false

penguins_weight <- penguins %>% pull(var = sex) %>% head(n = 10)
```

```{r}
# NA-Werte finden
is.na(penguins_weight)
```

Eine andere, intuitivere Methode ist, die Summe der fehlenden Werte für jede Spalte zu ermitteln. `is.na(df)` erzeugt eine logische Matrix, welche die NA-Positionen im Datenrahmen angibt. Die Funktion `colSums()` summiert dann die TRUE-Werte (die NA repräsentieren) in jeder Spalte und gibt die Anzahl der fehlenden Werte pro Spalte zurück.

```{r}
# Summe der NA-Werte pro Spalte
colSums(is.na(penguins))
```

`summarise_all()` wendet die Funktion `sum(is.na(.))` auf jede Spalte an (der Punkt steht hier für jede Spalte) und gibt die Anzahl der NA-Werte für jede Spalte zurück.

```{r}
penguins %>% 
  summarise_all(~ sum(is.na(.)))
```

Sie können auch die Summe der fehlenden Werte für jede Zeile erhalten. Dies kann bei kleinen Datensätzen nützlich sein.

```{r}
penguins %>% 
  filter(rowSums(is.na(.)) > 0)
```

Eine weitere Variante besteht darin, die fehlenden Zeilen mit der Funktion `everything()` zu filtern.

```{r}
penguins %>% 
  filter(if_any(everything(), is.na))
```

Ein anderer Ansatz zur Auswahl von Zeilen mit NA-Werten bzw. ohne NA ist die Verwendung der Funktion `complete.cases()`.

```{r}
# NA-Werte anzeigen
penguins[!complete.cases(penguins), ]
```

#### naniar

Die Funktion `vis_miss()` aus dem [naniar-Paket](https://naniar.njtierney.com) visualisiert das Muster der fehlenden Daten in Ihrem Datensatz und erleichtert so die Entscheidung, wie mit fehlenden Daten umzugehen ist.

### Fehlende Werte ersetzen

In R stehen für das Ersetzen von Werten und Löschen von Zeilen verschiedene Funktionen aus den Paketen tidyr und dplyr zur Verfügung. Beide Pakete sind in tidyverse enthalten.

Der Entscheid, ob fehlende Werte ersetzt oder die betroffenen Zeilen gelöscht werden, ist in erster Linie vom vorliegenden Datensatz abhängig. Bei umfangreichen Datensätzen ist ein Löschen von Zeilen weniger problematisch als bei solchen mit nur wenigen Beobachtungen.

Fehlende numerische Werte können durch die Lageparameter arithmetisches Mittel und Median der Variable oder durch die Zahl 0 ersetzt werden. Es ist für jede Spalte einzeln zu prüfen, welches Vorgehen sinnvoll ist.

#### Fehlende Werte durch Mittelwert ersetzen

Manchmal sagt ein Bild mehr als tausend Worte. Wenn Datenwissenschaftler eine Variable untersuchen (z.B. eine Stichprobe des Gewichts von Pinguinen), sind sie besonders an der Verteilung der Variable interessiert. Das heisst, sie wollen wissen, wie die verschiedenen Werte in der Stichprobe verteilt sind. Der Ausgangspunkt für diese Untersuchung ist oft die Visualisierung der Daten in Form eines Histogramms, um zu prüfen, wie häufig jeder Variablenwert auftritt.

```{r}
hist(penguins$body_mass_g, 
     breaks = 10)
```

```{r}
mean_weight <- mean(penguins$body_mass_g, na.rm = TRUE)
median_weight <- median(penguins$body_mass_g, na.rm = TRUE)
# Das Argument na.rm = TRUE wird ergänzt, um fehlende Werte für die Berechnung auszuschliessen.
cat("Mittelwert:", mean_weight, "\nMedian:", median_weight)
```

Das Erstellen und Modifizieren von Spalten übernimmt die Funktion `mutate()` aus dem Paket dplyr. Die allgemeine Struktur für das Hinzufügen oder Ändern von Spalten ist im Grunde dieselbe wie beim Filtern.

`df %>% mutate(neuer_spaltenname = was_sie_beinhaltet)`

```{r}
head(penguins)

penguins_mean <- penguins %>% 
  mutate(body_mass_g = replace_na(as.numeric(body_mass_g), mean(body_mass_g, na.rm = TRUE)))

head(penguins_mean)
```

#### Fehlende Werte durch Median ersetzen

Alternativ zum Mittelwert können die fehlenden Werte durch den Median der Variable «Age» ersetzt werden.

```{r}
penguins_median <- penguins %>% 
  mutate(body_mass_g = replace_na(as.numeric(body_mass_g), median(body_mass_g, na.rm = TRUE)))

head(penguins_median)
```

#### Fehlende Werte durch Wert 0 ersetzen

Fehlende Werte können Sie folgendermassen durch den Wert 0 ersetzen:

```{r}
penguins_0 <- penguins %>% 
  mutate(body_mass_g = replace_na(body_mass_g, 0))

head(penguins_0)
```

#### Fehlende Werte mit Replace-Funktion ersetzen

Die Funktion `replace()` ist ein praktisches Werkzeug in der R-Werkzeugkiste, um bestimmte Elemente in Vektoren und Datensets zu ändern. Sie ermöglicht es Ihnen, unerwünschte Werte durch neue zu ersetzen.

`replace(x, list, values)`

```{r}
mean_weight <- mean(penguins$body_mass_g, na.rm = TRUE) # Mittelwert ohne NA berechnen
new_penguins <- replace(x = penguins$body_mass_g, 
                        list = is.na(penguins$body_mass_g), 
                        values = mean_weight)

penguins$body_mass_g <- new_penguins # Datensatz aktualisieren
head(penguins$body_mass_g)
```

#### Zeilen mit fehlenden Werten entfernen

Zeilen mit fehlenden Werten können Sie mit der Funktion `drop_na()` aus dem Paket tidyr löschen.

```{r}
penguins_def <- penguins %>% drop_na()

head(penguins_def)
```

Eine alternative Funktion zum Entfernen von Zeilen mit fehlenden Werten ist `na.omit()`.

```{r}
penguins_def <- na.omit(penguins)

head(penguins_def)
```

```{r}
#| message: false
#| echo: false

kantone <- 
  c("Waadt", "Wallis", "Genf", "Bern", "Freiburg", "Solothurn", "Neuenburg", "Jura", 
    "Basel-Stadt", "Basel-Landschaft", "Aargau", "Zürich", "Glarus", "Schaffhausen", 
    "Appenzell A. Rh.", "Appenzell I. Rh.", "St. Gallen", "Graubünden", "Thurgau", "Luzern", 
    "Uri", "Schwyz", "Obwalden", "Nidwalden", "Zug", "Tessin")
```

```{r}
#| message: false
#| echo: false

bgb_staat_clean <- bgb_staat_clean %>% 
  filter(schulkanton %in% kantone)
```

```{r}
#| message: false
#| echo: false

bgb_typ_clean <- bgb_typ_clean %>% 
  filter(schulkanton %in% kantone)
```

```{r}
#| message: false
#| echo: false

region_asgmt <- function(bgb_datei) {
  bgb_datei %>% 
  mutate(region = case_when(
    schulkanton %in% c("Waadt", "Wallis", "Genf") ~ "Genferseeregion", 
    schulkanton %in% c("Bern", "Freiburg", "Solothurn", "Neuenburg", "Jura") ~ 
      "Espace Mittelland", 
    schulkanton %in% c("Basel-Stadt", "Basel-Landschaft", "Aargau") ~ "Nordwestschweiz", 
    schulkanton == "Zürich" ~ "Zürich", 
    schulkanton %in% c("Glarus", "Schaffhausen", "Appenzell A. Rh.", "Appenzell I. Rh.", 
                       "St. Gallen", "Graubünden", "Thurgau") ~ "Ostschweiz", 
    schulkanton %in% c("Luzern", "Uri", "Schwyz", "Obwalden", "Nidwalden", "Zug") ~ 
      "Zentralschweiz", 
    schulkanton == "Tessin" ~ "Tessin", 
    TRUE ~ "Andere Region"
  ))
}
```

```{r}
#| message: false
#| echo: false

bgb_staat_clean <- region_asgmt(bgb_staat_clean)
bgb_typ_clean <- region_asgmt(bgb_typ_clean)
```

### Zeilen filtern

Mit Funktionen aus dem Paket dplyr kann geprüft werden, ob ein bestimmter Wert in einer der Spalten vorkommt.

```{r}
penguins %>% 
  filter(if_any( # Bedingung für eine der Spalten erfüllt?
    everything(), ~ .x == 4500)) # Alle Spalten berücksichtigen
```

Base R bietet für das Auswählen von Zeilen eigene Funktionen. Mit `rowSums()` können die Zeilen mit dem angegebenen Wert identifiziert werden.

```{r}
filtered_rows <- which(rowSums(penguins == 4500) > 0, 
                       arr.ind = TRUE) # Ausgabe enthält Zeilen- und Spaltenindizes
penguins[filtered_rows, ]
```

Die Kombination der beiden Funktionen `filter()` und `grepl()` ermöglicht das Filtern von Zeilen anhand eines Musters.

```{r}
# Zeilen filtern, die «Chin» in der Spalte «species» enthalten
penguins %>% 
  filter(grepl(x = species, "Chin"))
```

Für die Suche nach mehreren Mustern gleichzeitig, können Sie die Funktionen `grepl()` und `paste()` kombinieren.

```{r}
bgb_typ_clean %>% 
  select(schulkanton) %>% 
  filter(grepl(paste(c("ern", "den"), collapse = "|"), schulkanton))
```

### Zeilen sortieren

Um die Zeilen eines Datensatzes zu sortieren, können Sie die Funktion `arrange()` aus dem Paket dplyr verwenden, welche die Zeilen eines Datenrahmens nach den Spaltenwerten sortiert.

```{r}
# Nach Gewicht aufsteigend sortieren
penguins %>% 
  select(species, island, body_mass_g, sex) %>% 
  arrange(body_mass_g) %>% 
  head(n = 10)

# Nach Gewicht absteigend sortieren
penguins %>% 
  select(species, island, body_mass_g, sex) %>% 
  arrange(desc(body_mass_g)) %>% 
  head(n = 10)
```

Eine alternative Variante ist das Verwenden der Funktion `order()` von Base R.

```{r}
penguins[order(penguins$bill_length_mm), ] %>% 
  head(n = 10)
```

### Zeilen entfernen

```{r}
# Zeilen bzw. Pinguine entfernen, die leichter als 3000 Gramm sind
penguins_filtered <- subset(penguins, body_mass_g <= 3000)

dim(penguins)
dim(penguins_filtered)
```

```{r}
# Zeilen nach Index entfernen
penguins_filtered <- penguins[-c(2, 4, 6), ]

head(penguins_filtered)
```

### Spalten auswählen

Datensätze enthalten oft mehr Informationen, als für eine bestimmte Analyse benötigt werden. Durch das Weglassen irrelevanter Spalten können Sie Ihre Daten straffen und sich auf das Wesentliche konzentrieren. Dies macht nicht nur den Code sauberer, sondern verbessert auch die Leistung bei der Arbeit mit grossen Datensätzen.

```{r}
penguins[, c(1, 2, 6)] %>% 
  head(n = 10)
```

```{r}
penguins %>% 
  select(species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>% 
  head(n = 10)

penguins %>% 
  select(species:body_mass_g) %>% 
  head(n = 10)
```

Select verfügt über eine Reihe von Hilfsfunktionen, mit denen Sie Variablen anhand ihrer Eigenschaften auswählen können. Zum Beispiel kann es sein, dass Sie nur an numerischen Merkmalen interessiert sind.

```{r}
penguins %>% 
  select(where(is.numeric)) %>% 
  head(n = 10)
```

Die Select-Funktion in Kombination mit `contains()` erleichtert die Auswahl von Spalten, welche eine bestimmte Zeichenfolge enthalten. Weitere Auswahlhilfen sind z.B. `starts_with()` oder `ends_with()`.

```{r}
penguins %>% 
  select(contains("length")) %>% 
  head(n = 10)
```

Mit einem vorangestellten Minuszeichen lassen sich Spalten aus dem Datensatz entfernen.

```{r}
penguins %>% 
  select(-island) %>% 
  head(n = 10)
```

Eine andere Möglichkeit zum Löschen von Spalten ist die Verwendung der Funktion `subset()`.

```{r}
penguins %>% 
  subset(select = -c(island, sex)) %>% 
  head(n = 10)
```

In R stehen weitere Optionen zur Verfügung, um bestimmte Spalten in einem Datensatz zu adressieren. Eine Variante ist der «Accessor» (Dollar-Notation) und eine andere ist das Verwenden der Pull-Funktion aus dem Paket dplyr.

```{r}
# Accessor
head(penguins$bill_length_mm, n = 5)
```

```{r}
# Pull-Funktion
head(pull(.data = penguins, all_of(bill_length_mm)), n = 5)
```

### Spalten zusammenführen

Das Kombinieren von Spalten in R ist eine gängige Operation bei der Arbeit mit Datensätzen. Die Funktion `unite()` ist dabei eine komfortable Möglichkeit, mehrere Spalten zu einer Spalte zusammenzufassen.

```{r}
# Mehrere Spalten vereinen und Originalspalten löschen
penguins %>% 
  unite(col = "penguins_gesamt", 
        species, 
        island, 
        year, 
        sep = ", ", 
        remove = TRUE) %>% 
  head()
```

```{r}
# Spalten zusammenführen mit paste()
paste(penguins$species, 
      "Pinguine leben auf", 
      penguins$island, 
      "und sind", 
      # Zahlen-, Faktor- oder Datumsspalten: Vor Kombi. mit as.character() in Zeichen umwandeln
      as.character(penguins$body_mass_g), 
      "Gramm schwer.") %>% 
  first()
```

### Datensätze zusammenführen

Das Zusammenführen mehrerer Datensätze ist eine wichtige Fähigkeit bei der Datenaufbereitung. Unabhängig davon, ob Sie mit kleinen oder grossen Datensätzen arbeiten, kann das Zusammenführen die Effizienz erheblich steigern.

#### cbind und rbind

```{r}
# Daten spaltenweise zusammenführen
cbined_df <- cbind(bgb_typ_clean, bgb_staat_clean[, 2:3])

head(cbined_df[, 7:10])

# Daten zeilenweise zusammenführen, sinnvoll bei gleichen Spalten
# rbind(sample1, sample2)
```

#### list2DF

Die Funktion list2DF ist nützlich für das Zusammenführen eines Datensatzes innerhalb einer Liste.

`list2DF(random_list)`

#### Datensätze auf Basis mehrerer Spalten zusammenführen

Das Zusammenführen von Datensätzen, die auf mehreren Spalten basieren, ist ein gängiger Vorgang in der Datenanalyse. Durch die Verwendung von Funktionen wie `merge()` können Sie Daten aus verschiedenen Quellen effizient kombinieren und gleichzeitig flexibel mit nicht übereinstimmenden Werten umgehen.

Inner Join kombiniert Zeilen aus beiden Datensätzen, die auf der Grundlage der angegebenen Spalten übereinstimmen. Zeilen mit nicht übereinstimmenden Werten werden ausgeschlossen.

```{r}
# Zur Veranschaulichung werden einige Regionen und Schulkantone durch «unbekannt» ersetzt.
bgb_staat_clean_ab <- bgb_staat_clean %>% 
  select(total, geschlecht_mann, geschlecht_frau, schulkanton, region) %>% 
  mutate(schulkanton = ifelse(schulkanton %in% c("Aargau", "Appenzell A. Rh.", 
                                                 "Appenzell I. Rh.", "Basel-Landschaft", 
                                                 "Basel-Stadt", "Bern"), 
                              yes = "unbekannt", no = schulkanton)) %>% 
   mutate(region = ifelse(schulkanton == "unbekannt", yes = "unbekannt", no = region))
```

```{r}
#| message: false
#| echo: false

bgb_typ_clean_efz_eba <- bgb_typ_clean %>% 
  select(total, typ_efz, typ_eba, schulkanton, region)
```

```{r}
# Auf Basis von «Region» und «Schulkanton» mit Inner Join zusammenführen
merge(x = bgb_staat_clean_ab, 
      y = bgb_typ_clean_efz_eba, 
      by = c("region", "schulkanton"))
```

Left Join behält alle Zeilen des linken Datensatzes (bgb_staat_clean) bei und fügt die entsprechenden Zeilen des rechten Datensatzes (bgb_typ_clean) ein. Wenn es keine Übereinstimmung gibt, werden NA-Werte für die Spalten von «bgb_typ_clean» eingefügt.

```{r}
# Auf Basis von «Region» und «Schulkanton» mit Left Join zusammenführen
merge(x = bgb_staat_clean_ab, 
      y = bgb_typ_clean_efz_eba, 
      by = c("region", "schulkanton"), 
      all.x = TRUE)
```

Right Join behält alle Zeilen des rechten Datensatzes (bgb_typ_clean) bei und fügt die entsprechenden Zeilen des linken Datensatzes (bgb_staat_clean) ein. Wenn es keine Übereinstimmung gibt, werden NA-Werte für die Spalten von «bgb_staat_clean» eingefügt.

```{r}
# Auf Basis von «Region» und «Schulkanton» mit Right Join zusammenführen
merge(x = bgb_staat_clean_ab, 
      y = bgb_typ_clean_efz_eba, 
      by = c("region", "schulkanton"), 
      all.y = TRUE)
```

Bei einem Full Join werden alle Zeilen aus beiden Datensätzen beibehalten, wobei für Spalten, für die keine Übereinstimmung besteht, NA-Werte verwendet werden.

```{r}
# Auf Basis von «Region» und «Schulkanton» mit Full Join zusammenführen
merge(x = bgb_staat_clean_ab, 
      y = bgb_typ_clean_efz_eba, 
      by = c("region", "schulkanton"), 
      all = TRUE)
```

### Kategoriale Variablen

Faktoren sind wichtige Datenstrukturen in R, die häufig zur Darstellung kategorialer Variablen verwendet werden. Sie speichern sowohl die Werte der kategorialen Variablen als auch die entsprechenden Stufen. Jede Faktorstufe repräsentiert eine eindeutige Kategorie innerhalb der Variablen.

#### Numerische in kategoriale Werte konvertieren

Bei manchen Variablen ist es sinnvoll, sie von einem numerischen Wert in eine kategoriale Grösse zu konvertieren. Aber auch nichtnumerische Variablen können in einen Faktor transformiert werden.

```{r}
class(penguins$year)

penguins_cat <- penguins %>% mutate(year = factor(year))

class(penguins_cat$year)
```

#### Numerische Variablen kategorisieren

Mit der Funktion `cut()` können kontinuierliche Variablen in Intervalle oder sogenannte «Bins» unterteilt werden, die auf bestimmten Messpunkten basieren. Auf diese Weise können Sie numerische Daten in kategorische Daten umwandeln, die sich leichter analysieren und interpretieren lassen.

`cut(x, breaks, labels = NULL, right = TRUE, ...)`

```{r}
gewichtsgruppen <- cut(x = penguins$body_mass_g, 
                       breaks = c(0, 3900, 5100, Inf), 
                       labels = c("leicht", "mittel", "schwer"))

tail(gewichtsgruppen)
```

#### Faktorstufen umbenennen

Die Umbenennung von Faktorstufen kann die Lesbarkeit und Interpretierbarkeit Ihrer kategorialen Daten erheblich verbessern. Das Paket forcats bietet dafür leistungsstarke Werkzeuge.

```{r}
penguins_cln <- penguins %>% 
  mutate(sex = fct_recode(sex, "M" = "male", "F" = "female"))

head(penguins_cln[, c(1:2, 7)])
```

### Textmanipulation

```{r}
# Beispiel-Text
text <- c("Fähigkeitszeugnis, Zeugnis, Zeugnisse, EFZ")
```

#### Auf vorhandene Zeichen prüfen

Bei der Arbeit mit Textdaten besteht eine häufige Aufgabe darin, zu prüfen, ob ein Zeichen oder eine Teilzeichenkette in einer längeren Zeichenkette enthalten ist. R stellt für diesen Zweck leistungsfähige Instrumente zur Verfügung, z.B. die Funktion `grepl()` von Base R, `str_detect()` von stringr oder `stri_detect_fixed()` von stringi.

```{r}
str_detect(string = text, 
           regex(pattern = "fähigkeitszeugnis", 
                 ignore_case = TRUE))
```

#### Auf vorhandene Teilstrings prüfen

```{r}
teilstrings <- c("Fähigkeitszeugnis", "EFZ")

# Prüfen, ob einzelne Werte wahr sind
str_detect(string = text, pattern = teilstrings)

# Prüfen, ob alle Werte wahr sind
all(str_detect(string = text, pattern = teilstrings))
```

#### Mustervergleich in Zeichenketten

Die Funktion `grep()` ist ein leistungsfähiges Werkzeug von Base R für Mustervergleiche und das Suchen in Zeichenketten. Standardmässig führt `grep()` einen Teilabgleich durch, was zu unerwarteten Ergebnissen führen kann, wenn Sie nach exakten Übereinstimmungen suchen. Eine effektive Methode für den exakten Abgleich ist die Verwendung der Anker \^ (Anfang der Zeichenfolge) und \$ (Ende der Zeichenfolge). Dadurch wird sichergestellt, dass der gesuchte Begriff die gesamte Zeichenkette ist und nicht nur ein Teil davon.

```{r}
# Beispiel-Text
text <- c("Fähigkeitszeugnis", "Zeugnis", "Zeugnisse", "EFZ")
```

```{r}
grep(pattern = "^Zeugnis$", 
     x = text)
# Weitere Argumente: ignore.case = TRUE, value = TRUE
```

```{r}
# Reguläre Ausdrücke für komplexe Suchmuster
grep(pattern = "^[ZE]", 
     x = text)
```

```{r}
# Suchmuster mit OR kombinieren
grep(pattern = "Zeugnis|EFZ", 
     x = text)
```

Bei der Arbeit mit grossen Datensätzen kann die Leistung der verschiedenen Abgleichsmethoden erheblich sein. Im Allgemeinen ist die Verwendung von `==` oder `%in%` für exakte Vergleiche in einfachen Fällen schneller. `grep()` ist jedoch effizienter, wenn Sie mit komplexen Mustern arbeiten oder wenn Sie die zusätzlichen Funktionen wie ignore.case oder die value-Optionen verwenden wollen.

```{r}
any(text == "Zeugnis")
text[text == "Zeugnis"]
```

#### Ungefähre Übereinstimmung finden

Die Funktion `agrep()` von Base R wird für die annähernde Übereinstimmung von Zeichenketten verwendet, auch bekannt als Fuzzy Matching.

Die Funktion ist besonders nützlich für

-   Suche nach ähnlichen Zeichenfolgen in einem Datensatz

-   Durchführung unscharfer Suchen in Textfeldern

-   Korrektur von Rechtschreibfehlern in Textdaten

`agrep()` ist ein leistungsfähiges Werkzeug für das Fuzzy Matching, aber es ist wichtig, geeignete Parameter zu wählen (insbesondere max.distance), um die richtige Balance zwischen dem Erkennen relevanter Übereinstimmungen und dem Vermeiden falsch positiver Ergebnisse zu finden. Bei sehr umfangreichen Abgleichsaufgaben kann die Verwendung der Funktion langsam sein.

```{r}
agrep(pattern = "zügnisse", 
      x = text, 
      max.distance = 0.4) # Maximal zulässige Distanz für einen Treffer
```

#### Zeichenketten ver­bin­den

Beim Zusammenfügen von Zeichenketten werden zwei oder mehrere Elemente miteinander verbunden. Dabei spielt es keine Rolle, ob Sie mit Textdaten arbeiten oder dynamische Ausgaben erzeugen.

```{r}
text <- str_c(bgb_typ_clean$typ_efz[1], 
              " Lernende wurden im Kanton [",  
              bgb_typ_clean$schulkanton[1], 
              "] mit einem eidg. Fähigkeitszeugnis (EFZ) ausgezeichnet.", 
              sep = "") # Ohne Separator ist str_c() äquivalent zu paste0()

text
```

#### Zeichenkette aufteilen und Element extrahieren

```{r}
ergebnis <- str_split(string = text, 
                      pattern = " ", 
                      simplify = FALSE) # TRUE: Rückgabe als Matrix

sapply(X = ergebnis, `[`, 1)
```

#### Zeichenkette zwischen bestimmten Zeichen extrahieren

Die Funktion `str_extract()` extrahiert die erste Teilzeichenkette, die einem Regex-Muster entspricht. Sie verwendet Lookbehind `(?<=\\[)` und Lookahead `(?=\\])`, um Text zwischen \[ und \] zu finden und einen einfachen Abgleich für Text zwischen ( und ) durchzuführen.

```{r}
# Text zwischen eckigen Klammern extrahieren
str_extract(string = text, 
            pattern = "(?<=\\[).*?(?=\\])") # Alternative: "\\[.*?\\]"
```

```{r}
# Alle Übereinstimmungen extrahieren
str_extract_all(string = text, 
                pattern = "(?<=\\[).*?(?=\\])|(?<=\\().*?(?=\\))")
```

#### Zeichenkette vor Leerzeichen extrahieren

Um den Teil der Zeichenkette vor dem ersten Leerzeichen zu finden und zu extrahieren, können Sie `str_extract()` mit einem regulären Ausdruck verwenden. Das Muster `^[^ ]+` entspricht dem Anfang der Zeichenkette (`^`), gefolgt von einem oder mehreren Zeichen, die keine Leerzeichen sind (`[^ ]+`).

```{r}
# Zeichenkette vor erstem Leerzeichen extrahieren
str_extract(string = text, 
            pattern = "^[^ ]+")
```

#### Zahlen aus Zeichenkette extrahieren

Drei Methoden im Vergleich:

-   Base R ist flexibel und benötigt keine zusätzlichen Pakete, aber die Syntax kann etwas umständlich sein.

-   stringr, Bestandteil von tidyverse, vereinfacht den Prozess mit intuitiven Funktionen, so dass der Code leichter zu lesen und zu schreiben ist.

-   stringi bietet leistungsfähige und effiziente String-Operationen, die sich für leistungskritische Aufgaben eignen.

```{r}
zahlen <- str_extract_all(string = text, 
                          pattern = "\\d+") # \\d+ extrahiert eine oder mehrere Zahlen
as.numeric(unlist(zahlen))
```

#### Zahl in Ziffern aufteilen

```{r}
# Funktion zum Aufteilen einer einzelnen Zahl in Ziffern
split_number <- function(number){
  number_str <- as.character(number)
  number_with_spaces <- gsub(pattern = "(.)", 
                             replacement = "\\1 ", # Leerzeichen zwischen den Ziffern einfügen
                             x = number_str)
  digits <- strsplit(x = number_with_spaces, split = " ")[[1]]
  as.numeric(digits)
}
```

```{r}
# Funktion auf jede Zahl des Vektors anwenden
lapply(X = zahlen, 
       FUN = split_number)
```

#### Führende Nullen bei Zahlen hinzufügen

Manchmal ist es erforderlich, dass Zahlen ein bestimmtes Format haben. Das Hinzufügen von führenden Nullen ist eine Möglichkeit, die erforderliche Konsistenz der Datendarstellung zu gewährleisten.

```{r}
ergebnis <- sprintf("%05d", bgb_typ_clean$typ_eba)
# %d = Ganzzahl, 05 = Ausgabe 5 Zeichen lang

head(bgb_typ_clean$typ_eba)
head(ergebnis)
```

#### Zeichen ersetzen

```{r}
# Zur Veranschaulichung wird der Begriff «Genferseeregion» durch zusätzlichen Text ergänzt.
bgb_typ_clean_region <- bgb_typ_clean %>% 
  mutate(region = ifelse(region == "Genferseeregion", 
                         yes = "Region 1201 Genferseeregion", 
                         no = region)) %>% 
  filter(region == "Region 1201 Genferseeregion") %>% 
  pull(region)

bgb_typ_clean_region
```

Die Funktion `sub()` ersetzt die erste Übereinstimmung in einer Zeichenfolge durch neue Zeichen, während die Funktion `gsub()` alle Übereinstimmungen in einer Zeichenfolge durch neue Zeichen ersetzt.

```{r}
# Erste Übereinstimmung ersetzen
bgb_typ_clean_region %>% 
  sub(pattern = "region", 
      replacement = "", 
      ignore.case = TRUE)

# Alle Übereinstimmungen ersetzen
bgb_typ_clean_region %>% 
  # Für verschiedene Muster (pattern) den Operator | verwenden
  gsub(pattern = "region", # Auch reguläre Ausdrücke möglich, z.B. ".*\\$" (bis/mit $-Zeichen)
       replacement = "", 
       ignore.case = TRUE)
```

### Ausreisser identifizieren

#### Ausreisser visualisieren

Mit der Funktion «stats_dist» lassen sich die Verteilung und eine zusammenfassende Statistik für eine spezifische Spalte anzeigen.

```{r}
#| message: false
#| echo: false

# Theme DataVisual erstellen

# Konstante
schrift_farbe <- "#2d2926"

theme_dv <- function(...) {
  theme(text = element_text(family = "Manrope", 
                            colour = schrift_farbe), 
        
        # Dezentes Raster hinzufügen
        panel.grid.major = element_line(colour = "#dbd9ce", 
                                        linewidth = 0.2), 
        panel.grid.minor = element_blank(), 
        
        # Hintergrundfarben
        legend.background = element_rect(fill = "#f5f5f2", 
                                         colour = NA), 
        panel.background = element_rect(fill = "#f5f5f2", 
                                        colour = NA), 
        plot.background = element_rect(fill = "#f5f5f2", 
                                       colour = NA), 
        
        # Ränder und Margin
        panel.border = element_blank(), 
        panel.spacing = unit(x = c(-4, 8, 8, 8), 
                             units = "pt"), 
        plot.margin = unit(x = c(19, 19, 8, 19), 
                           units = "pt"), 
        
        # Titel
        legend.text = element_text(colour = schrift_farbe, 
                                   size = rel(1.1), 
                                   hjust = 0), 
        legend.title = element_text(face = "bold", 
                                    colour = schrift_farbe, 
                                    size = rel(1.3)), 
        plot.title = element_text(face = "bold", 
                                  colour = schrift_farbe, 
                                  size = rel(1.75), 
                                  hjust = 0.5), 
        plot.subtitle = element_text(face = "bold", 
                                     colour = schrift_farbe, 
                                     size = rel(1.5), 
                                     hjust = 0.5, 
                                     margin = margin(t = -4, b = -4, l = 8, 
                                                     unit = "pt"), 
                                     debug = FALSE), 
        
        # Achsenbeschriftung
        axis.title = element_text(colour = schrift_farbe, 
                                  size = rel(1.3), 
                                  hjust = 0.5), 
        axis.text = element_text(colour = schrift_farbe, 
                                 size = rel(1.1)), 
        
        # Bildlegenden
        plot.caption = element_text(colour = "#5d5a59", 
                                    size = rel(0.9), 
                                    hjust = 0.5, 
                                    margin = margin(t = 8, b = 0, 
                                                    unit = "pt")), 
        ...
  )
}
```

```{r}
#| echo: false

stats_dist <- function(var_data, binwidth) {
  
  # Zusammenfassende Statistiken abrufen, indem Werte aus Spalte extrahiert werden
  min_val <- min(pull(var_data))
  max_val <- max(pull(var_data))
  mean_val <- mean(pull(var_data))
  median_val <- median(pull(var_data))
  modal_val <- mfv(pull(var_data))
  
  # Statistik ausgeben
  stats <- glue(
    "Minimum: {format(round(min_val, 2), nsmall = 2)}
    Mittelwert: {format(round(mean_val, 2), nsmall = 2)}
    Median: {format(round(median_val, 2), nsmall = 2)}
    Modus: {format(round(modal_val, 2), nsmall = 2)}
    Maximum: {format(round(max_val, 2), nsmall = 2)}"
  )
  
  # Histogramm erstellen
  hist <- var_data %>% 
    ggplot(aes(x = pull(var_data))) +
    geom_histogram(binwidth = binwidth, 
                   fill = "#93257B", 
                   alpha = 0.6, 
                   boundary = 0.4) +
    scale_x_continuous(limits = c(30, 60)) +
    
    # geom_vline() fügt eine vertikale Referenzlinie zu einem Diagramm hinzu
    geom_vline(xintercept = min_val, color = "#57AF2C", linetype = "dashed", 
               linewidth = 1) +
    geom_vline(xintercept = mean_val, color = "#9FC131", 
               linewidth = 1) +
    geom_vline(xintercept = median_val, color = "black", 
               linewidth = 1) +
    geom_vline(xintercept = modal_val, color = "#BD304C", 
               linewidth = 1) +
    geom_vline(xintercept = max_val, color = "#57AF2C", linetype = "dashed", 
               linewidth = 1) +
    
    # Titel und Beschriftungen hinzufügen
    labs(title = "Datenverteilung", 
         x = "", 
         y = "Häufigkeit") +
    theme_minimal(base_size = 10) +
    theme_dv()
  
  # Boxplot erstellen
  boxplt <- var_data %>% 
    ggplot(aes(x = pull(var_data), y = 1)) +
    geom_boxplot(fill = "#9FC131", 
                 color = "#5d5a59", 
                 alpha = 0.7) +
    scale_x_continuous(limits = c(30, 60)) +
    
    # Titel und Beschriftungen hinzufügen
    labs(x = "Wert",
         y = "") +
    theme_minimal(base_size = 10) +
    theme_dv(panel.grid.major.y = element_blank())
  
  # Liste verwenden, um mehrere Ausgaben zurückzugeben
  return(
    
    # Histogramm und Boxplot mit Hilfe von Patchwork kombinieren
    list(stats,
         hist / boxplt)
  ) # Ende der zurückgegebenen Ausgaben
} # Ende der Funktion
```

```{r}
#| warning: false

# Spalte auswählen
df_col <- penguins %>% 
  select(bill_length_mm) %>% 
  drop_na()

# Aufruf der Funktion stats_dist
stats_dist(df_col, 2)
```

#### Ausreisser mit Z-Score erkennen

Der Z-Score bezieht sich auf die Anzahl der Standardabweichungen der einzelnen Datenwerte vom Mittelwert. Ein Z-Score von Null entspricht dem exakten Mittelwert. Ein üblicher Schwellenwert für die Identifizierung von Ausreissern ist ein Z-Score grösser als 3 oder kleiner als -3.

```{r}
z_score <- scale(penguins$body_mass_g)
ausreisser <- abs(z_score) > 3

head(ausreisser)
```

### Ausreisser bereinigen

Variablen können auf der Grundlage eines bestimmten Perzentils angepasst werden. Perzentile sind Lageparameter, die einen der Grösse nach geordneten Datensatz in 100 gleich grosse Teile zerlegen. Sie unterteilen den Datensatz also in 1%-Schritte.

Um sicherzustellen, dass keine wertvollen Informationen gelöscht werden, sollte das Entfernen eines bestimmten Perzentils der Daten sorgfältig geprüft werden.

```{r}
# Quantile, die 1% und 99% der Daten entsprechen
pcntile01 <- penguins %>% 
  pull(bill_length_mm) %>% 
  quantile(probs = 1/100, names = FALSE, na.rm = TRUE)

pcntile99 <- penguins %>% 
  pull(bill_length_mm) %>% 
  quantile(probs = 99/100, names = FALSE, na.rm = TRUE)

# 1. und 99. Perzentil ausgeben
cat("Minimum:", min(penguins$bill_length_mm, na.rm = TRUE), "\n1. Perzentil:", pcntile01, 
    "\n99. Perzentil:", pcntile99, "\nMaximum:", max(penguins$bill_length_mm, na.rm = TRUE))
```

Durch das Erstellen einer Funktion können Sie das Entfernen von Ausreissern automatisieren.

```{r}
remove_outliers <- function(data) {
  data_cleaned <- data
  for (variable in names(data)) {
    z_score <- scale(data)
    outliers <- abs(z_score) > 3
    data_cleaned <- data[!apply(X = outliers, MARGIN = 1, FUN = any), ]
  }
  return(data_cleaned)
}
```

```{r}
# Funktion «remove_outliers» anwenden
remove_outliers(data = penguins$bill_length_mm) %>% 
  head()
```

## Ausführliche Analyse

### Bootstrapping

Angenommen, Sie haben einen Datensatz und möchten den Mittelwert einer Variable verstehen. Wird dieser durch wenige Ausreisser verzerrt, ist der Aussagewert jedoch gering.

Bootstrapping ist eine statistische Technik, bei der mehrere Kopien der Daten erstellt werden, von denen jede eine leichte Abweichung aufweist. Die Statistik, die Sie interessiert (z.B. der Mittelwert), wird dann für jede Kopie berechnet.

Die Funktion `bootstrap_stat_plot()` berechnet und visualisiert die Verteilung, so dass Sie ein klares Bild davon erhalten, wie die Statistik zwischen den verschiedenen Versionen der Daten variiert. Detaillierte Infos zur Funktion finden Sie auf der dazugehörenden [Website](https://www.spsanderson.com/TidyDensity/reference/bootstrap_stat_plot.html).

```{r}
#| warning: false

x <- penguins$body_mass_g # Variable bestimmen
ns <- 1000 # Anzahl Simulationen festlegen

# Mittelwert
var_gewicht_1 <- tidy_bootstrap(.x = x, .num_sims = ns) %>% 
  bootstrap_stat_plot(.value = y, 
                      .stat = "cmean", 
                      .show_groups = TRUE, 
                      .show_ci_labels = TRUE, 
                      .interactive = FALSE)

# Minimum
var_gewicht_2 <- tidy_bootstrap(.x = x, .num_sims = ns) %>% 
  bootstrap_stat_plot(.value = y, 
                      .stat = "cmin", 
                      .show_groups = TRUE, 
                      .show_ci_labels = TRUE, 
                      .interactive = FALSE)

# Maximum
var_gewicht_3 <- tidy_bootstrap(.x = x, .num_sims = ns) %>% 
  bootstrap_stat_plot(.value = y, 
                      .stat = "cmax", 
                      .show_groups = TRUE, 
                      .show_ci_labels = TRUE, 
                      .interactive = FALSE)

# Standardabweichung
var_gewicht_4 <- tidy_bootstrap(.x = x, .num_sims = ns) %>% 
  bootstrap_stat_plot(.value = y, 
                      .stat = "csd", 
                      .show_groups = TRUE, 
                      .show_ci_labels = TRUE, 
                      .interactive = FALSE)
wrap_plots(var_gewicht_1, var_gewicht_2, var_gewicht_3, var_gewicht_4, 
           ncol = 2, 
           nrow = 2, 
           widths = c(1, 1), 
           heights = c(1, 1))
```

### Korrelation prüfen

Die Korrelation ist ein statistisches Mass, das angibt, inwieweit zwei Variablen in einer linearen Beziehung zueinander stehen (d.h. sich in einem festen Verhältnis zueinander verändern).

```{r}
# Metrisch skalierte Variablen
cor(x = penguins$bill_length_mm, 
    y = penguins$bill_depth_mm, 
    use = "complete.obs", 
    method = "pearson") # Alternativen: Spearmans Rho und Kendalls Tau für ordinale Variablen
```

Unter der Voraussetzung einer Normalverteilung und der Annahme, dass es sich bei den vorliegenden Daten um eine Stichprobe aus einer Grundgesamtheit (Population) handelt, kann mit `cor.test()` die Signifikanz geprüft werden.

```{r}
cor.test(x = penguins$bill_length_mm, 
         y = penguins$bill_depth_mm, 
         method = "pearson")
```

```{r}
# Datensatz auf hoch korrelierte Merkmale prüfen
penguins_def %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(method = "circle", 
           addCoef.col = "white", 
           order = "hclust", 
           addrect = 2, 
           rect.col = "grey")
```

### Gruppierte Zusammenfassung

In R können gruppierte Zusammenfassungen mit `group_by() %>% summarise()` erstellt werden. `group_by()` ändert dabei die Analyseeinheit vom gesamten Datensatz zu einzelnen Gruppen. `summarise()` erstellt einen neuen Datenrahmen für die zusammenfassende Statistik.

```{r}
penguins_def %>% 
  group_by(species) %>% 
  summarise(Mittelwert_Gewicht = mean(body_mass_g)) %>% 
  arrange(desc(Mittelwert_Gewicht))
```

Angenommen, Sie haben viele numerische Spalten und möchten die Mittelwertfunktion auf alle Spalten anwenden. Mit `across()` ist es einfach, eine Funktion für mehrere Spalten zu verwenden.

```{r}
penguins_def %>% 
  select(-year) %>% 
  group_by(species) %>% 
  summarise(across(where(is.numeric), mean)) # Alt.: summarise(across(c(Var1, Var2), mean))
```

#### Top-Werte finden

Bei der Datenanalyse besteht oft die Notwendigkeit, die besten Werte innerhalb jeder Gruppe eines Datensatzes zu extrahieren. Unabhängig davon, ob es sich um Verkaufsdaten, Umfrageantworten oder eine andere Art von gruppierten Daten handelt, kann die Identifizierung der Top-Werte oder Ausreisser innerhalb jeder Gruppe wertvolle Erkenntnisse liefern.

```{r}
penguins_def %>% 
  select(-c(bill_depth_mm:year)) %>% 
  group_by(species) %>% 
  top_n(n = 1, bill_length_mm) %>% 
  arrange(species, desc(bill_length_mm))
```

#### Zweithöchster Wert finden

Der zweithöchste Wert einer Variable wird in R mit folgendem Code gefunden.

```{r}
sort(x = penguins_def$bill_length_mm)[length(penguins_def$bill_length_mm) - 1]
```

#### Prozent- und Absolutwerte ausgeben

Dank `tabyl()` aus dem Paket janitor können Prozent- und Absolutwerte zusammen ausgegeben werden. Mit `adorn()` lässt sich `tabyl()` zudem leicht anpassen. Sie können z.B. Summen hinzufügen oder die Anzahl der Nachkommastellen für Prozentwerte festlegen.

```{r}
penguins_def %>% 
  tabyl(var1 = species) %>% 
  # Gesamtsumme hinzufügen
  adorn_totals() %>% 
  # Prozentsätze formatieren
  adorn_pct_formatting() %>% 
  as_tibble()
```

```{r}
#| message: false
#| echo: false

# penguins_def %>% 
  # tabyl(var1 = island, var2 = species) %>% 
  # Gesamtsumme hinzufügen
  # adorn_totals() %>% 
  # Anzahl in Prozent umrechnen
  # adorn_percentages() %>% 
  # Prozentsätze formatieren
  # adorn_pct_formatting(digits = 2) %>% 
  # Anzahl und Prozentwerte ausgeben
  # adorn_ns() %>% 
  # Titelzeilen hinzufügen
  # adorn_title() 
  # %>% as_tibble(.name_repair = "minimal")
```

#### Apply-Familie

Mit `tapply()` können Sie eine Funktion auf Untergruppen anwenden. Dies können in R vorhandene Funktionen wie `mean()` oder `sd()` sein, aber auch von Ihnen selbst geschriebene Funktionen.

`tapply(X, INDEX, FUN, simplify = TRUE)`

```{r}
tapply(X = penguins_def$bill_length_mm, # Numerischer Wert
       INDEX = penguins_def$species, # Faktor
       FUN = mean) # Funktion
```

Die Funktion `tapply()` kann auch zusammen mit `summary()` verwendet werden, um einen schnellen Überblick über die Verteilung einer Variable innerhalb von Gruppen zu erhalten.

```{r}
tapply(X = penguins_def$bill_length_mm, 
       INDEX = penguins_def$species, 
       FUN = summary, 
       na.rm = TRUE) # Optional
```

```{r}
schnabellaenge <- function(bill, avg_bill) {
  bill > avg_bill
}

# Pinguine finden, die einen längeren Schnabel haben als der Durchschnitt
tapply(X = head(penguins_def$bill_length_mm, n = 15), 
       INDEX = head(penguins_def$species, n = 15), 
       mean(penguins_def$bill_length_mm), 
       FUN = schnabellaenge)
```

#### describeBy

Die Funktion `describeBy()` aus dem psych-Paket berechnet verschiedene Kennwerte für jede Faktorstufe.

```{r}
describeBy(x = penguins_def$bill_length_mm, penguins_def$species, mat = TRUE)
```

### Numerische Variablen vergleichen

Um zwei numerische Variablen zu vergleichen, werden die Daten zunächst mit `pivot_longer()` in ein Langformat umgewandelt.

```{r}
#| warning: false

# Daten von breit nach lang transformieren
penguins_long <- penguins_def %>% 
  select(-c(bill_length_mm, bill_depth_mm, sex, year)) %>% 
  pivot_longer(cols = flipper_length_mm:body_mass_g, 
               names_to = "variable", 
               values_to = "wert")

head(penguins_long, n = 10)
```

#### Numerische Variablen in Balkendiagramm

```{r}
options(scipen = 999)

penguins_long %>% 
  group_by(species, variable) %>% 
  summarise(summe_lv = sum(wert), .groups = "keep") %>% 
  ggplot() +
  geom_bar(mapping = aes(x = species, y = summe_lv, fill = variable), 
           alpha = 0.7, 
           stat = "identity", 
           position = position_dodge(width = 0.9)) +
  scale_y_continuous(labels = comma_format(big.mark = "'")) +
  scale_fill_paletteer_d("nbapalettes::pacers_classic") +
  labs(title = "Pinguine im Vergleich", 
       x = "Pinguinart", 
       y = "Wert") +
  theme_minimal(base_size = 10) +
  theme_dv(axis.text.x = element_text(angle = 90, 
                                      hjust = 1), 
           panel.grid.major.y = element_line(color = "#5d5a59", 
                                             linetype = "dashed", 
                                             linewidth = 0.5)) +
  theme(legend.title = element_blank())
```

#### Stack und Unstack

Das Stapeln von numerischen Variablen ist auch mit Hilfe der Funktion `stack()` möglich.

```{r}
penguins_stack <- penguins_def %>% 
  select(bill_length_mm:flipper_length_mm) %>% 
  stack()

head(penguins_stack)
```

Für das Entstapeln wird die Funktion `unstack()` verwendet.

```{r}
unstack(penguins_stack) %>% 
  head()
```

#### Numerische Variablen normalisieren

Wenn die Werte, wie im obigen Beispiel, auf unterschiedlichen Skalen liegen, sind sie nicht ohne weiteres vergleichbar. Eine gebräuchliche Technik im Umgang mit numerischen Daten besteht darin, diese so zu normalisieren, dass die Werte ihre proportionale Verteilung behalten, aber auf derselben Skala gemessen werden. Zu diesem Zweck wird eine Technik namens Min/Max-Skalierung verwendet, bei der die Werte proportional auf einer Skala von 0 bis 1 verteilt werden.

```{r}
# group_by() stellt sicher, dass die Variablen unabhängig voneinander normalisiert werden.
penguins_normalized <- penguins_long %>% 
  group_by(variable) %>% 
  mutate(wert = rescale(wert, to = c(0, 1)))

head(penguins_normalized)
```

Alternativ können numerische Variablen im Datensatz mit der Funktion `scale()` normalisiert werden. Jede Variable wird so standardisiert, dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 hat.

```{r}
penguins_normalized_scale <- penguins_long %>% 
  mutate(wert_normalisiert = scale(wert))
  
head(penguins_normalized_scale)
```

Vergleichen Sie die numerischen Variablen erneut in einem Balkendiagramm. Diesmal wird jedoch der Datensatz mit den normalisierten Werten verwendet.

```{r}
options(scipen = 999)

penguins_normalized %>% 
  group_by(species, variable) %>% 
  summarise(summe_lv = sum(wert), .groups = "keep") %>% 
  ggplot() +
  geom_bar(mapping = aes(x = species, y = summe_lv, fill = variable), 
           alpha = 0.7, 
           stat = "identity", 
           position = position_dodge(width = 0.9)) +
  scale_y_continuous(labels = comma_format(big.mark = "'")) +
  scale_fill_paletteer_d("nbapalettes::pacers_classic") +
  labs(title = "Pinguine im Vergleich", 
       x = "Pinguinart", 
       y = "Wert") +
  theme_minimal(base_size = 10) +
  theme_dv(axis.text.x = element_text(angle = 90, 
                                      hjust = 1), 
           panel.grid.major.y = element_line(color = "#5d5a59", 
                                             linetype = "dashed", 
                                             linewidth = 0.5)) +
  theme(legend.title = element_blank())
```

## Datenvisualisierung

### Bedeutung

Die Visualisierung von Daten ist eine effiziente Methode, neues Wissen zu entdecken und dieses Nicht-Experten mit Hilfe visueller Darstellungen auf eine zugängliche Weise zu vermitteln.

### ggplot2

ggplot2 ist ein Paket zur Erstellung eleganter Datenvisualisierungen in R.

`ggplot(data = df) + geom_col(mapping = aes(x = Variable_1, y = Variable_2))`

Eine Visualisierung initialisieren Sie mit der Funktion `ggplot()` und dem Datensatz, der für die Darstellung verwendet werden soll. `ggplot(data = df)` erstellt im Grunde ein leeres Diagramm, dem Sie mittels Pluszeichen (+) Ebenen hinzufügen können.

`geom_col()` fügt dann eine Ebene von Balken hinzu, deren Höhe den Variablen entspricht, die durch das Mapping-Argument angegeben sind. Das Argument mapping ist immer mit `aes()` gekoppelt, das bestimmt, wie die Variablen auf der X- und Y-Achse abgebildet werden.

```{r}
options(scipen = 999)

# Kombiniertes Histogramm und Dichtediagramm erstellen
penguins_def %>% 
  ggplot(mapping = aes(x = body_mass_g)) +
  geom_histogram(mapping = aes(y = after_stat(density)), 
                 bins = 10, 
                 color = "black", 
                 fill = "white") +
  geom_density(color = "#9FC131", 
               fill = "#9FC131", 
               linewidth = 1, 
               alpha = 0.2) +
  labs(title = "Histogramm und Dichtediagramm", 
       x = NULL, 
       y = "Dichte") +
  theme_minimal(base_size = 10) +
  theme_dv()
```

#### Mehrere Plots zusammen ausgeben

Mit der Funktion `par()` lassen sich mehrere Plots nebeneinander ausgegeben. Dies ist hilfreich, wenn man beispielsweise die Verteilung mehrerer Variablen vergleichen möchte.

```{r}
par(mfrow = c(2, 2))

hist(penguins_def$bill_length_mm)
hist(penguins_def$bill_depth_mm)
hist(penguins_def$flipper_length_mm)
hist(penguins_def$body_mass_g)
```

```{r}
#| message: false
#| echo: false

dev.off()
```

### Vergrössertes Säulendiagramm

```{r}
# Daten für Beispiel aufbereiten
mittlere_schnabellaenge_df <- 
  as.data.frame(mittlere_schnabellaenge) %>% 
  rownames_to_column()

colnames(mittlere_schnabellaenge_df) <- c("species", "mean_bill_length_mm")
```

Für die Darstellung von Diagrammen kann in R eine eigene Farbpalette definiert werden.

```{r}
# Eigene, benutzerdefinierte Farbpalette erstellen
palette <- c("#9FC131", "#93257B", "#57AF2C")
```

Wenn kleine Abweichungen im Säulendiagramm von Bedeutung sind, wird empfohlen, beides darzustellen. Ein normales Säulendiagramm, das bei Null beginnt, aber auch eine vergrösserte Version, die es dem Leser ermöglicht, die einzelnen Säulenwerte besser zu erkennen. Das folgende Beispiel zeigt das normale Säulendiagramm auf der rechten Seite und eine vergrösserte Version, die einen Teil der Achse zeigt, auf der linken Seite. Die Verbindungslinien zwischen den beiden Versionen zeigen, auf welchen Datenbereich sich die abgeschnittene Achse bezieht.

Ähnlich kann verfahren werden, wenn eine Säule viel länger ist als die anderen, so dass es schwierig ist, alle kleinen Säulen zu unterscheiden.

```{r}
mittlere_schnabellaenge_df %>% 
  ggplot(mapping = aes(x = species, y = mean_bill_length_mm, fill = species)) +
  geom_col() +
  facet_zoom(ylim = c(35, 50)) +
  scale_fill_manual(values = palette) +
  scale_y_continuous(breaks = seq(0, 50, by = 10), 
                     labels = seq(0, 50, by = 10), 
                     limits = c(0, 50), 
                     expand = c(0, 0)) +
  labs(title = "Vergrössertes Balkendiagramm der Pinguinarten", 
       x = "", 
       y = "Mittlere Schnabellänge") +
  theme_dv(legend.position = "none") +
  theme(axis.text = element_text(size = rel(0.9)), 
        panel.spacing = unit(x = c(0, 8, 0, -3.25), 
                             units = "pt"))
```

### Gestapeltes Balkendiagramm

Ein gestapeltes Balkendiagramm wird verwendet, um die kumulierten Prozentsätze pro Gruppe anzuzeigen.

```{r}
# Daten für Beispiel aufbereiten
bgb_typ_clean_stacked <- bgb_typ_clean %>% 
  select(starts_with("typ"), region) %>% 
  pivot_longer(cols = !region, names_to = "typ", values_to = "wert")

bgb_typ_clean_stacked$typ[bgb_typ_clean_stacked$typ == "typ_nicht_bbg_reglementierte_berufliche_grundbildung"] <- "typ_nicht_reglementiert"
```

```{r}
bgb_typ_clean_stacked %>% 
  na.omit() %>% 
  ggplot(mapping = aes(x = factor(region), y = wert, fill = factor(typ))) +
  geom_bar(stat = "identity", position = "fill") +
  scale_fill_manual(values = palette, na.value = "#5d5a59") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Gestapeltes Balkendiagramm nach Region", 
       x = "Region", 
       y = "Prozentualer Anteil", 
       fill = "Typ") +
  theme_minimal(base_size = 10) +
  theme_dv() +
  coord_flip()
```

```{r}
#| message: false
#| echo: false

# Fehlerbalkendiagramm; zuerst Paket Hmisc installieren

# ggplot(data = penguins_def, 
       # mapping = aes(x = body_mass_g, y = island)) +
  # stat_summary(geom = "point", fun = mean, size = 2) +
  # labs(x = "", y = "Gewicht") +
  # guides(colour = "none") +
  # stat_summary(geom = "errorbar", 
               # position = position_dodge(width = 0.9), 
               # fun.data = mean_cl_normal, 
               # width = 0.2)
```

### Dichteplot

In der Statistik geht es oft darum, Stichproben von Daten zu nehmen und mithilfe von Wahrscheinlichkeitsfunktionen Informationen über die gesamte Datenpopulation (Grundgesamtheit) zu extrapolieren. Mit einer ausreichenden Anzahl dieser Zufallsvariablen können Sie eine sogenannte Wahrscheinlichkeitsdichtefunktion berechnen, welche die Verteilung der Variable für die gesamte Population schätzt.

Ein Dichteplot ist eine Darstellung der Verteilung einer numerischen Variable. Es handelt sich um eine geglättete Version des Histogramms und wird häufig in der gleichen Situation verwendet. `geom_density()` berechnet und zeichnet eine solche Kernel-Dichte-Schätzung.

```{r}
#| echo: false

options(scipen = 999)

show_dens <- function(var_data) {
  
  # Statistiken abrufen
  mean_val <- mean(pull(var_data))
  median_val <- median(pull(var_data))
  modal_val <- mfv(pull(var_data))
  
  # Dichteplot erstellen
  dens_plt <- var_data %>% 
    ggplot(aes(x = pull(var_data))) +
    geom_density(fill = "#93257B", 
                 color = "white", 
                 alpha = 0.4) +
    
    # Linien für die Statistik
    geom_vline(xintercept = mean_val, 
               color = "#9FC131", 
               linetype = "dashed", 
               linewidth = 1.3) +
    geom_vline(xintercept = median_val, 
               color = "#BD304C", 
               linetype = "dashed",
               linewidth = 1.3) +
    geom_vline(xintercept = modal_val, 
               color = "#57AF2C", 
               linetype = "dashed",
               linewidth = 1.3) +
    
    # Titel und Beschriftungen
    labs(title = "Datendichte", 
         x = "", 
         y = "Dichte") +
    theme_minimal(base_size = 10) +
    theme_dv()
  
  return(dens_plt) # Ende der zurückgegebenen Ausgaben
} # Ende der Funktion
```

```{r}
# Spalte auswählen
df_col <- penguins_def %>% 
  select(body_mass_g)

# Aufruf der Funktion show_dens
show_dens(var_data = df_col)
```

### Boxplot

#### Boxplot mit Streudiagramm

```{r}
penguins_def %>% 
  ggplot(mapping = aes(x = body_mass_g, y = 1)) +
  stat_dist_halfeye(fill = "#93257B") +
  geom_point(mapping = aes(y = 0.75), 
             shape = 21, 
             colour = "#9FC131", 
             size = 4, 
             fill = "#9FC131", 
             alpha = 0.7, 
             position = position_jitter(height = 0.1, 
                                        seed = 1234)) +
  geom_boxplot(width = 0.25, 
               colour = "black", 
               linewidth = 1) +
  labs(title = "Datenverteilung", 
       x = "Wert", 
       y = element_blank()) +
  theme_minimal(base_size = 10) +
  theme_dv(legend.position = "none", panel.grid.major.y = element_blank()) +
  coord_cartesian(xlim = range(penguins_def$body_mass_g))
```

#### Regenwolkenplot

Das Regenwolkendiagramm ist eine Kombination aus Boxplot und Violinplot mit einem Histogramm zur detaillierten Darstellung der Daten.

```{r}
penguins_def %>% 
  ggplot(mapping = aes(x = body_mass_g, y = 1)) +
  stat_halfeye(fill = "#93257B") +
  stat_dots(mapping = aes(y = 0.8), 
            colour = "#9FC131", 
            fill = "#9FC131", 
            side = "bottom") +
  geom_boxplot(width = 0.25, 
               colour = "black", 
               linewidth = 1) +
  labs(title = "Datenverteilung", 
       x = "Wert", 
       y = element_blank()) +
  theme_minimal(base_size = 10) +
  theme_dv(legend.position = "none", panel.grid.major.y = element_blank()) +
  coord_cartesian(xlim = range(penguins_def$body_mass_g))
```

### Streudiagramm

#### Symmetrisches Streudiagramm

Bei der Darstellung der Residuen eines linearen Regressionsmodells wird u. a. darauf geachtet, wie die Punkte um die Nulllinie verteilt sind. Bei Residuendiagrammen führen die Standardeinstellungen oft zu einer y-Achse, die nicht symmetrisch um 0 ist. Dies kann die Vergleichbarkeit erschweren. Wenn die y-Achse symmetrisch um den Nullpunkt gelegt und eine Nulllinie hinzugefügt wird, ist es einfacher zu erkennen, ob mehr Punkte über oder unter dem Nullpunkt liegen.

```{r}
# Daten für Beispiel aufbereiten
penguins_def_adelie <- 
  penguins_def %>% 
  filter(species == "Adelie")

# Modell erstellen
fit <- lm(formula = body_mass_g ~ bill_length_mm, data = penguins_def_adelie)

fit_df <- broom::augment(fit)
```

```{r}
fit_df %>% 
  ggplot(mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, 
             colour = "#9FC131") +
  scale_y_continuous(limits = c(-1200, 1200), 
                     expand = c(0, 0)) +
  labs(title = "", 
       x = "", 
       y = "") +
  theme_minimal(base_size = 10) +
  theme_dv()
```

#### Streudiagramm mit Quantilsanzeige

Das Quantil kann für den ausgewählten Datensatz und die Variable flexibel gewählt werden.

```{r}
#| echo: false

highlight_points <- function(var_data, df_col, quantiles) {
  var_data %>% 
    mutate(highlight = between(x = df_col, 
                               left = quantile(df_col, quantiles[1]), 
                               right = quantile(df_col, quantiles[2]))) %>% 
    ggplot(mapping = aes(x = df_col, y = "", fill = highlight)) +
    geom_point(shape = 21, 
               colour = "#9FC131", 
               size = 4, 
               alpha = 0.7, 
               position = position_jitter(width = 0, 
                                          height = 0.25, 
                                          seed = 1234)) +
    labs(title = "Streudiagramm", 
         x = "Wert",
         y = element_blank()) +
    theme_minimal(base_size = 10) +
    theme_dv(legend.position = "none", panel.grid.major.y = element_blank()) +
    scale_fill_manual(values = c("#9FC131", "#93257B"))
}
```

```{r}
highlight_points(var_data = penguins_def, 
                 df_col = penguins_def$bill_length_mm, 
                 quantiles = c(0.25, 0.5))
```

### Streumatrix

Mit einem Streumatrixdiagramm, bestehend aus Histogrammen und Trendlinien, können Sie Zusammenhänge sichtbar machen.

```{r}
#| echo: false

panel_hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col = "#93257B", ...)
}
```

```{r}
#| warning: false

pairs(penguins_def[3:6], 
      main = "Streumatrix", 
      panel = panel.smooth,
      cex = 1, pch = 21, bg = "#9FC131",
      diag.panel = panel_hist, cex.labels = 1.5, font.labels = 2)
```

### Treemap

Eine Treemap stellt hierarchische Daten als eine Reihe von verschachtelten Rechtecken dar. Jede Gruppe wird durch ein Rechteck dargestellt, dessen Fläche proportional zu ihrem Wert ist.

```{r}
penguins_def %>% 
  group_by(island, species) %>% 
  summarise(mittelwert_gewicht = mean(body_mass_g)) %>% 
  treemap(index = c("island", "species"), 
          vSize = "mittelwert_gewicht", 
          vColor = "island", 
          type = "categorical", 
          title = "Verteilung der Pinguinarten", 
          palette = alpha(palette, alpha = 0.5), # Alt.: brewer.pal(n = 3, name = "Pastel1")
          fontsize.title = 17.5, 
          fontsize.labels = 13, 
          bg.labels = 0, 
          align.labels = list(c("left", "top"), c("right", "bottom")), 
          position.legend = "none")
```

### Multidimensionale Skalierung

Die Visualisierung von Ähnlichkeiten zwischen Datenpunkten kann schwierig sein, insbesondere wenn es sich um viele Merkmale handelt. Hier kommt die multidimensionale Skalierung (MDS) ins Spiel. Sie ermöglicht die Untersuchung dieser Beziehungen in einem niedrigdimensionalen Raum, typischerweise 2D oder 3D, um die Interpretation zu erleichtern.

```{r}
# Relevante numerische Merkmale auswählen (für eine bestimmte Anzahl Zeilen)
mds_features <- penguins_def[11:20, c(3:6)]

# Paarweise Abstände zwischen Merkmalen berechnen
mds_matrix <- dist(mds_features)

# head(mds_matrix)
```

```{r}
# MDS durchführen, um eine 2D-Darstellung zu erhalten
mds_results <- cmdscale(d = mds_matrix, k = 2)

# head(mds_results, n = 3)
```

```{r}
# R-Basisplot erstellen
plot(x = mds_results[, 1], y = mds_results[, 2], 
     main = "Multidimensionale Skalierung für Pinguine", 
     xlab = "Dimension 1", 
     ylab = "Dimension 2")

# Textbeschriftungen hinzufügen (optional)
text(mds_results, labels = penguins_def$species, col = "#9FC131", cex = 0.62, pos = 1)
```

### Dynamische Grafiken

Ergänzend zu den statischen Datenvisualisierungen mit `plot()` oder ggplot2 gibt es für R verschiedene Pakete, mit denen Sie interaktive und dynamische Grafiken erstellen können.

Beispiele:

-   [echarts4r](https://echarts4r.john-coene.com)
-   [gganimate](https://gganimate.com/#gganimate-)
-   [Plotly](https://plotly.com)

### Interaktive Tabellen

#### gt

Ob für Berichte, Präsentationen oder wissenschaftliche Arbeiten, [gt](https://gt.rstudio.com) ermöglicht die Erstellung optisch ansprechender Tabellen mit detaillierter Kontrolle über die Formatierung und ist damit ein unverzichtbares Werkzeug für die professionelle Präsentation Ihrer Daten.

#### reactable

Das [reactable](https://glin.github.io/reactable/)-Paket basiert auf der JavaScript-Bibliothek «React Table» und ist ein flexibles Werkzeug zur Erstellung interaktiver und anpassbarer Tabellen in Webanwendungen und R Shiny Dashboards.

# Abschliessende Worte

## Fazit

Durch die Verwendung der sechs Verben (filtern, anordnen, auswählen, mutieren, gruppieren und zusammenfassen), die Sie in dieser Anleitung gelernt haben, sind Sie auf dem besten Weg, die meisten Herausforderungen bei der Datenanalyse in R zu lösen. Datenvisualisierungen mit ggplot2 erleichtern zudem das Erkennen komplexer Zusammenhänge während der Datenanalyse und schaffen Klarheit bei der Präsentation vor dem Auftraggeber.

## Ausblick auf Machine Learning

### Prolog

Maschinelles Lernen wird in R mit dem Paket caret oder dem tidymodels-Framework (<https://www.tidymodels.org>) realisiert. Ein einfacher Ansatz für maschinelles Lernen (ML) ist z.B. die Erstellung eines linearen Regressionsmodells mit der Funktion `lm()`.

### tidyAML

tidyAML ist ein R-Paket, das es ermöglicht, das tidymodels-Ökosystem zu nutzen, um automatisiertes maschinelles Lernen (AutoML) durchzuführen, insbesondere wenn keine Feinabstimmung erforderlich ist. Mit tidyAML können Benutzer viele hochwertige maschinelle Lernmodelle und Vorhersagen auf einmal erstellen. Ein Sicherheitsmechanismus stellt sicher, dass das Paket korrekt fehlschlägt, wenn die erforderlichen Erweiterungspakete nicht installiert sind.

Es ist möglich, die gewünschten Modelle auszuwählen, indem man entweder die unterstützten Parsnip-Funktionen wie `linear_reg()` oder die gewünschte Engine auswählt; man kann auch beide zusammen verwenden.

```{r}
fast_regression_parsnip_spec_tbl(.parsnip_fns = "linear_reg", 
                                 .parsnip_eng = c("lm", "glm"))
```

### Lineare Regression

Die Funktion `fast_regression()` verwendet Ihre Daten und das Rezeptobjekt, um mehrere Regressionsmodelle mit einer Vielzahl von Engines und Funktionen aus dem Paket parsnip zu generieren.

```{r}
rec_obj <- recipe(formula = mpg ~ ., data = mtcars) #  Vorverarbeitungsschritte festlegen

rec_obj
```

```{r}
fast_reg_tbl <- fast_regression(.data = mtcars, 
                                .rec_obj = rec_obj, 
                                .parsnip_fns = "linear_reg", # Standardwert: "all"
                                .parsnip_eng = c("lm", "glm")) # Standardwert: "all"
# Weitere Funktionsargumente:
# .split_type: (Standardwert: "initial_split"), .split_args, .drop_na (Standardwert: TRUE)

fast_reg_tbl
```

Mit der Funktion `extract_regression_residuals()` lassen sich die Residuen der angepassten Modelle zusammen mit den ursprünglichen und den vorhergesagten Daten extrahieren.

```{r}
extract_regression_residuals(.model_tbl = fast_reg_tbl)
```

Die Qualität der Modelle kann durch Visualisierung mit den beiden Funktionen `plot_regression_residuals()` und `plot_regression_predictions()` überprüft werden. Damit können die Residuen sowie ein Vergleich zwischen Vorhersage und Wirklichkeit dargestellt und mögliche Probleme identifiziert werden.

```{r}
extract_regression_residuals(.model_tbl = fast_reg_tbl) %>% 
  plot_regression_residuals()
```

```{r}
extract_wflw_pred(.data = fast_reg_tbl, 
                  .model_id = 1:nrow(fast_reg_tbl)) %>% 
  plot_regression_predictions()
```

## Quarto

Quarto ermöglicht es, Inhalte und ausführbaren Code in einem Dokument zu kombinieren. Mehr über Quarto erfahren Sie unter <https://quarto.org>.
